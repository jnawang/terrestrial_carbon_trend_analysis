---
title: "detect abrupt change"
output: pdf_document
date: "2025-02-07"
author: "Junna Wang"
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=TRUE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r raw data location}
rm(list=ls())
# change this location where the raw data will be saved accordingly
# dir_rawdata <- '/Users/jw2946/Documents/stability_project/SiteData/'
dir_rawdata <- '/Volumes/MaloneLab/Research/Stability_Project/SiteData'
#
library(librarian)
shelf(tidyverse, Kendall, Rbeast, cowplot, ggpubr, tidyterra, ggpubr, viridis, RColorBrewer, grid, terra, trend)
#
```


```{r look at all annual NEE data }
####I need to pay attention to Reynold watershed sites####

sites_long <- read.csv('data/long_term_ec_sites.csv')
rm_years   <- read.csv('data/years_to_remove.csv')
#
files.unzip.Ameri <- list.files(file.path(dir_rawdata, "Ameriflux_FLUXNET", "unzip"), full.names = T)
files.unzip.Europ2020 <- list.files(file.path(dir_rawdata, "FLUXNET2020", "unzip"), full.names = T)
files.unzip.Europ2023 <- list.files(file.path(dir_rawdata, "ICOS_FLUXNET", "unzip_connect2020"), full.names = T)
files.Ameri.ReddyProcGapFill <- list.files(file.path(dir_rawdata, "Ameriflux_BASE", "ReddyProcGapFill"), full.names=T)
#
a_YY_all <- data.frame(site_ID=character(), TIMESTAMP=integer(), NEE_VUT_REF=double())
#
sites_long$nyear <- 0
#
for (i in 1:nrow(sites_long)) {
#  i = 53
  print(i)
  site_ID <- sites_long$site_ID[i]
  print(site_ID)
  if (sites_long$source[i] == "AmeriFlux_FLUXNET") {
    a_YY <- read.csv(files.unzip.Ameri[grepl(pattern=paste0(site_ID, "_FLUXNET_FULLSET_YY"), files.unzip.Ameri)])
  } else if (sites_long$source[i] == "EuropFlux_FLUXNET2023") {
    a_YY <- read.csv(files.unzip.Europ2023[grepl(pattern=paste0(site_ID, "_FLUXNET_YY"), files.unzip.Europ2023)])
  } else if (sites_long$source[i] == "EuropFlux_FLUXNET2020") {
    a_YY <- read.csv(files.unzip.Europ2020[grepl(pattern=paste0(site_ID, "_FLUXNET2015_FULLSET_YY"), files.unzip.Europ2020)])
  } else if (sites_long$source[i] == "AmeriFlux_BASE") {
    a_YY <- read.csv(files.Ameri.ReddyProcGapFill[grepl(pattern=paste0(site_ID, "_EDDYPROC_FULLSET_YY"), files.Ameri.ReddyProcGapFill)])
  }
  a_YY[a_YY==-9999] <- NA
  #
  a_YY_good <- a_YY %>% filter(!TIMESTAMP %in% rm_years$years[rm_years$site_ID==site_ID])
  sites_long$nyear[i] <- sum(!is.na(a_YY_good$NEE_VUT_REF))
  #
  # plot(a_YY_good$TIMESTAMP, a_YY_good$NEE_VUT_REF, main=site_ID)
  # plot(a_YY_good$TIMESTAMP, a_YY_good$GPP_NT_VUT_REF, main=site_ID)
  # plot(a_YY_good$TIMESTAMP, a_YY_good$RECO_NT_VUT_REF, main=site_ID)
  #
  a_YY_all <- rbind(a_YY_all, data.frame(site_ID=site_ID, a_YY_good[, c("TIMESTAMP", "NEE_VUT_REF")]))
}
#
ggplot(data=a_YY_all, aes(x=TIMESTAMP, y=NEE_VUT_REF, color=site_ID)) +
  geom_point() +
  geom_line() +
  labs(x='Year', y='Net ecosystem exchange (g C/m2/year)') + 
  theme(legend.position = "none")

# get site_years; it will be good to see this. where are these data?
a_YY_all %>% group_by(TIMESTAMP) %>% summarise(nSites=n()) %>% 
  ggplot(aes(x=TIMESTAMP, y=nSites)) + 
  geom_point() +
  labs(x='Year', y='Number of sites') +
  theme_bw()
#
###############################################
# make a plot for locations of these sites
mundi <- vect(rnaturalearthdata::coastline110)
mundi <- crop(mundi,ext(c(-180,180,-60,90)))
#
#
p1 <- ggplot() +
  geom_spatvector(data = mundi, linewidth = .2) +
  geom_hline(yintercept=0, linetype = "dotted", colour = "grey") + 
  geom_point(data=sites_long, aes(x=Long, y=Lat, fill=category), shape = 21, size = 2, stroke = 0.5, color='black') +
  scale_fill_manual(values=c('wet' = "#4393C3", 'dry' = "#E4003A", 'cold' = "#B790D4"), 
                    labels=c("wet"='Wet: temperate and tropical climates', 
                             "dry"="Dry: semi-arid climate",
                             "cold"="Cold: semi-arid climate")) + 
  labs(x='Longitude', y='Latitude', fill='Climate category') +
  coord_sf(expand=FALSE, xlim = c(-180, 90), ylim = c(-50, 90)) + 
  theme_bw() +
  theme(panel.grid.minor = element_blank(), panel.grid.major = element_blank(), panel.border = element_rect(colour = "black")) +
  theme(legend.position = c(0.25, 0.2),
        legend.text = element_text(size = 12), 
        legend.background = element_rect(fill = alpha('white', 0.3)))
# p1

# number of sites for each climate category
sites_long %>% group_by(category) %>% summarise(nSites=n())
# 
tmp <- a_YY_all %>% group_by(site_ID) %>% summarise(nYears=n()) 

# only 25 sites with > 18 year data; only 21 sites with > 19 year data. 
tmp %>%  #    filter(nYears > 18)
  ggplot(aes(x=nYears)) +
  geom_histogram() + 
  labs(x='Number of years', y='Number of sites') +
  theme_bw()


```


```{r calculate abrupt changes of NEE, fig.width=15, fig.height=10}
# it takes 10-15 mins! 
# Note that, different running may have different results about whether selecting DE-Hai. because this is a Bayesian method. 
# How many abrupt changes do we have in NEE time series?
# If a time series is less than 14 years, I do not need to cut it into two segments, because one segment will be too short to use. 
# I only use the section with more than 7 years data
threshold_nyear_ac <- 8
sites_long <- read.csv('data/long_term_ec_sites.csv')
#
files.unzip.Ameri <- list.files(file.path(dir_rawdata, "Ameriflux_FLUXNET", "unzip"), full.names = T)
files.unzip.Europ2020 <- list.files(file.path(dir_rawdata, "FLUXNET2020", "unzip"), full.names = T)
files.unzip.Europ2023 <- list.files(file.path(dir_rawdata, "ICOS_FLUXNET", "unzip_connect2020"), full.names = T)
files.Ameri.ReddyProcGapFill <- list.files(file.path(dir_rawdata, "Ameriflux_BASE", "ReddyProcGapFill"), full.names=T)
#
#### trend outcome: only use linear regression. 
#### I should have a column called period. 
trend_result <- data.frame(site_ID=character(), ncp=integer(), cp=I(list()), cpPr=I(list()), cpAbruptChange=I(list()), Ytrend=I(list()))
for (i in 1:nrow(sites_long)) {
#  i = 53
  print(i)
  site_ID <- sites_long$site_ID[i]
  trend_result[i, 1] <- site_ID
  # AmeriFlux and European flux; Do I need to differentiate them? Hopefully they use the same notation
  if (sites_long$source[i] == "AmeriFlux_FLUXNET") {
#    a_HH <- read.csv(files.unzip.Ameri[grepl(pattern=paste0(site_ID, "_FLUXNET_FULLSET_HH"), files.unzip.Ameri)])
    a_DD <- read.csv(files.unzip.Ameri[grepl(pattern=paste0(site_ID, "_FLUXNET_FULLSET_DD"), files.unzip.Ameri)])
    a_YY <- read.csv(files.unzip.Ameri[grepl(pattern=paste0(site_ID, "_FLUXNET_FULLSET_YY"), files.unzip.Ameri)])
  } else if (sites_long$source[i] == "EuropFlux_FLUXNET2020") {
#    a_HH <- read.csv(files.unzip.Europ2020[grepl(pattern=paste0(site_ID, "_FLUXNET2015_FULLSET_HH"), files.unzip.Europ2020)])
    a_DD <- read.csv(files.unzip.Europ2020[grepl(pattern=paste0(site_ID, "_FLUXNET2015_FULLSET_DD"), files.unzip.Europ2020)])
    a_YY <- read.csv(files.unzip.Europ2020[grepl(pattern=paste0(site_ID, "_FLUXNET2015_FULLSET_YY"), files.unzip.Europ2020)])
  } else if (sites_long$source[i] == "EuropFlux_FLUXNET2023") {
#    a_HH <- read.csv(files.unzip.Europ2023[grepl(pattern=paste0(site_ID, "_FLUXNET_HH"), files.unzip.Europ2023)])
    a_DD <- read.csv(files.unzip.Europ2023[grepl(pattern=paste0(site_ID, "_FLUXNET_DD"), files.unzip.Europ2023)])
    a_YY <- read.csv(files.unzip.Europ2023[grepl(pattern=paste0(site_ID, "_FLUXNET_YY"), files.unzip.Europ2023)])
  } else if (sites_long$source[i] == "AmeriFlux_BASE") {
#    a_HH <- read.csv(files.Ameri.ReddyProcGapFill[grepl(pattern=paste0(site_ID, "_EDDYPROC_FULLSET_HH"), files.Ameri.ReddyProcGapFill)])
    a_DD <- read.csv(files.Ameri.ReddyProcGapFill[grepl(pattern=paste0(site_ID, "_EDDYPROC_FULLSET_DD"), files.Ameri.ReddyProcGapFill)])
    a_YY <- read.csv(files.Ameri.ReddyProcGapFill[grepl(pattern=paste0(site_ID, "_EDDYPROC_FULLSET_YY"), files.Ameri.ReddyProcGapFill)])
  }
  # a_HH[a_HH==-9999] <- NA
  a_DD[a_DD==-9999] <- NA
  a_YY[a_YY==-9999] <- NA
  #
  a_DD$TIMESTAMP <- as.Date(as.character(a_DD$TIMESTAMP), "%Y%m%d")
  #
  # do I need to remove some years for this site
  # if (Site.ID %in% rm_years$Site.ID) {
  #   years <- rm_years$years[rm_years$Site.ID == Site.ID]
  #   #
  #   a_DD <- a_DD %>% filter(!year(TIMESTAMP) %in% years)
  #   a_YY <- a_YY %>% filter(!TIMESTAMP %in% years)
  #   # MannKendall analysis: this analysis violates the basic assumption!
  # }
  # if (length(unique(a_YY$TIMESTAMP)) < 8) {
  #   next
  # }
  #
  trend_beast <- beast.irreg(y=a_DD$NEE_VUT_REF, time=a_DD$TIMESTAMP, deltat='1d', season='harmonic', period='1.0 year', tcp.minmax = c(0, 3), torder.minmax=c(1, 1), tseg.min = threshold_nyear_ac*365)  # change point should be longer than 7 years; reference: tseg.min = 7,  https://www.rdocumentation.org/packages/Rbeast/versions/0.9.5/topics/beast
  plot(trend_beast)   # i do not know how to use this?
  p1 <- recordPlot()
  # dev.copy(png, paste0("beast_trend/", Site.ID, '.png'))
  # dev.off()
  # how are these trends consistent with our prediction and understanding?
  # useful information to output:
  # trend_beast$trend$ncp; trend_beast$trend$cp; the year of break point; trend_beast$trend$cpPr; probability
  # trend_beast$trend$cpAbruptChange; trend is in the trend_beast$trend$Y
  trend_result$ncp[i]    <- trend_beast$trend$ncp
  trend_result$cp[[i]]   <- trend_beast$trend$cp
  trend_result$cpPr[[i]] <- trend_beast$trend$cpPr    
  # ncpPr: the probability distribution of have 0, 1, and tcp.max change point. 
  trend_result$cpAbruptChange[[i]] <- trend_beast$trend$cpAbruptChange
#  trend_result[i, 42] <- trend_beast$trend$Y
#### another way of getting trends is to use losses
  p2 <- ggplot(data=a_YY, aes(x=TIMESTAMP, y=NEE_VUT_REF)) +
    geom_point() +
    geom_smooth(method = 'loess') +
    labs(tile=site_ID)
  #
  p3 <- ggplot(data=a_YY, aes(x=TIMESTAMP, y=NEE_VUT_REF)) +
  geom_point() +
  stat_cor(color='green') +
  geom_smooth(method = 'lm') +
  labs(tile=site_ID)
  #
  p23 <- plot_grid(p2, p3, nrow=2, ncol=1)
  #
  plot_grid(p1, p23, nrow=1, ncol=2)
  #
  ggsave(paste0("graphs/beast_trend/", site_ID, '.png'))
#
}
# a new data frame to store abrupt change information
abrupt_change <- data.frame(site_ID=character(), ncp=integer(), cpAbruptChange=vector())
#
index0 <- which(!is.na(trend_result$cpPr))
for (id in index0) {
  ncp <- 0
  years <- c()
  Pr  <- trend_result$cpPr[id][[1]]
  for (icp in 1:length(Pr)) {
    if (!is.na(Pr[icp]) & Pr[icp] > 0.95) {
      ncp <- ncp + 1
      
      if (trend_result$cp[id][[1]][icp] - floor(trend_result$cp[id][[1]][icp]) < 0.05) {
        years <- c(years, round(trend_result$cp[id][[1]][icp]))
      } else {
        years <- c(years, ceiling(trend_result$cp[id][[1]][icp]))
      }
    }
  }
  if (ncp > 0) {
    abrupt_change <- rbind(abrupt_change, data.frame(site_ID=trend_result$site_ID[id], ncp=ncp, cpAbruptChange=years))
  }
  #
}

# sometimes "US-Cbo" is broken up ~2013 which is too close to end year; use another break point 2008, close to midpoint 
abrupt_change$cpAbruptChange[abrupt_change$site_ID == "CA-Cbo"] <- 2008

# "US-PFa" is broken up around 2007 because of long-term drought. 
abrupt_change$cpAbruptChange[abrupt_change$site_ID == "US-PFa"] <- 2007
# "US-PFa" is broken up at 2010 because US-Ho1 is also broken in this year; and data quality at the identified 2015 year is too bad. 
abrupt_change$cpAbruptChange[abrupt_change$site_ID == "US-Ho2"] <- 2010   
# the identified year 2015 is not measured well. 

# "US-Syv" has 3-4 years missing data around 2010, use larger break-point
abrupt_change$cpAbruptChange[abrupt_change$site_ID == "US-Syv"] <- 2016 

# turning point at 2013 and the year looks abnormal
abrupt_change$cpAbruptChange[abrupt_change$site_ID == "SE-Deg"] <- 2012

# use 2010 instead of 2006 because data before 2000 is not useful. 
abrupt_change$cpAbruptChange[abrupt_change$site_ID == "BE-Bra"] <- 2010

# it has 2-3 missing data in later years, so use the first break point
abrupt_change$cpAbruptChange[abrupt_change$site_ID == "IT-MBo"] <- 2013

# use 2013 instead of 2016, because the later one is too extreme
abrupt_change$cpAbruptChange[abrupt_change$site_ID == "US-Uaf"] <- 2013

# 2010 is more likely for FI-Hyy
abrupt_change$cpAbruptChange[abrupt_change$site_ID == "FI-Hyy"] <- 2010

# IL-Yat swings between 2008 and 2009
abrupt_change$cpAbruptChange[abrupt_change$site_ID == "IL-Yat"] <- 2009

# CH-Dav swings between 2015 and 2016
abrupt_change$cpAbruptChange[abrupt_change$site_ID == "CH-Dav"] <- 2015

# "US-ME2" sometime can find abrupt change, sometimes cannot, so to be consistent, we used no abrupt change for this site
if ("US-Me2" %in% abrupt_change$site_ID) {
  abrupt_change <- abrupt_change %>% filter(site_ID != "US-Me2")
}
# "US-Los" has lots of missing years. and sometime it was selected and sometimes not. 
if ("US-Los" %in% abrupt_change$site_ID) {
  abrupt_change <- abrupt_change %>% filter(site_ID != "US-Los")
}
#
write.csv(abrupt_change, 'data/abrupt_change_years.csv', row.names = F)

hist(abrupt_change$cpAbruptChange)

```


```{r reorganize the time series, fig.width==8, fig.height=4}
# a function to calculate sens slope
cal_sens_slope <- function(x) {
  x <- x[!is.na(x)]
  x <- scale(x)
  return(as.numeric(sens.slope(x)[c(1,3)]))
}

# In this chunk, I want to get: all the sub-time series, start year, end year, and simple linear trend of each sub-time series
threshold_nyear <- 7
#
abrupt_change <- read.csv('data/abrupt_change_years.csv')
sites_long    <- read.csv('data/long_term_ec_sites.csv')
# How many sub time series do we have?
rm_years   <- read.csv('data/years_to_remove.csv')
#
# a data frame about sub time series
sub_time_series <- data.frame(site_ID=character(), year_start=integer(), year_end=integer(), 
                              NEE=double(), NEEtrend_lm=double(), NEEp=double(), 
                              GPP=double(), GPPtrend_lm=double(), GPPp=double(), 
                              ER=double(), ERtrend_lm=double(), ERp=double())
#
data_NEE_trend <- data.frame(NEE=double(), year=integer(), site_ID=character(), IGBP=character(), subts_ID=integer())
#
files.unzip.Ameri <- list.files(file.path(dir_rawdata, "Ameriflux_FLUXNET", "unzip"), full.names = T)
files.unzip.Europ2020 <- list.files(file.path(dir_rawdata, "FLUXNET2020", "unzip"), full.names = T)
files.unzip.Europ2023 <- list.files(file.path(dir_rawdata, "ICOS_FLUXNET", "unzip_connect2020"), full.names = T)
files.Ameri.ReddyProcGapFill <- list.files(file.path(dir_rawdata, "Ameriflux_BASE", "ReddyProcGapFill"), full.names=T)
#
# set NA option for linear regression
options(na.action = na.omit)
#
isub_ts <- 1
for (i in 1:nrow(sites_long)) {
  # i = 15
  print(i)
  site_ID <- sites_long$site_ID[i]
  print(site_ID)
  if (sites_long$source[i] == "AmeriFlux_FLUXNET") {
    a_YY <- read.csv(files.unzip.Ameri[grepl(pattern=paste0(site_ID, "_FLUXNET_FULLSET_YY"), files.unzip.Ameri)])
  } else if (sites_long$source[i] == "EuropFlux_FLUXNET2020") {
    a_YY <- read.csv(files.unzip.Europ2020[grepl(pattern=paste0(site_ID, "_FLUXNET2015_FULLSET_YY"), files.unzip.Europ2020)])
  } else if (sites_long$source[i] == "EuropFlux_FLUXNET2023") {
    a_YY <- read.csv(files.unzip.Europ2023[grepl(pattern=paste0(site_ID, "_FLUXNET_YY"), files.unzip.Europ2023)])
  } else if (sites_long$source[i] == "AmeriFlux_BASE") {
    a_YY <- read.csv(files.Ameri.ReddyProcGapFill[grepl(pattern=paste0(site_ID, "_EDDYPROC_FULLSET_YY"), files.Ameri.ReddyProcGapFill)])
  }
  a_YY[a_YY==-9999] <- NA
  #
  a_YY_good <- a_YY %>% filter(!TIMESTAMP %in% rm_years$years[rm_years$site_ID==site_ID])
  #
  year_start <- min(na.omit(a_YY_good$TIMESTAMP))
  year_end   <- max(na.omit(a_YY_good$TIMESTAMP))
  #
  # think about that if there is no abrupt change. 
  if (site_ID %in% abrupt_change$site_ID) {
    years_abrupt_change <- abrupt_change$cpAbruptChange[abrupt_change$site_ID == site_ID]
    # sort years from small to big. 
    years_abrupt_change <- sort(years_abrupt_change)
    #
    for (iyear_ac in 1:length(years_abrupt_change)) {
      if (years_abrupt_change[iyear_ac] - year_start + 1 >= threshold_nyear - 1) {
        # put it into data frame: sub_time_series
        sub_time_series[isub_ts, "site_ID"] <- site_ID
        sub_time_series[isub_ts, "year_start"] <- year_start
        sub_time_series[isub_ts, "year_end"] <- years_abrupt_change[iyear_ac] - 1
        a_YY_sub <- a_YY_good %>% filter(TIMESTAMP >= sub_time_series[isub_ts, "year_start"] & 
                                         TIMESTAMP <= sub_time_series[isub_ts, "year_end"]) %>% 
                                  filter(!is.na(NEE_VUT_REF)) %>% 
                                  mutate(NEE_scale = scale(NEE_VUT_REF), year_center = scale(TIMESTAMP, scale=FALSE))
        sub_time_series[isub_ts, "NEE"] <- mean(a_YY_sub$NEE_VUT_REF, na.rm=T)
        sub_time_series[isub_ts, c("NEEtrend_lm", "NEEp")] <- cal_sens_slope(a_YY_sub$NEE_scale)
          # summary(lm(data=a_YY_sub, NEE_scale ~ year_center))$coefficients[2, c(1,4)]
        #
        # use daytime GPP and ER when it is available, otherwise use other time series
        # remove GPP and ER outliers; 
        if (sum(!is.na(a_YY_sub$GPP_DT_VUT_REF)) >= 5 & !site_ID %in% c("BR-Sa1", "US-NC2", "US-SP3", "US-WCr", "US-Ho1", "US-Ho2", "US-NC4", "US-Kon", "US-MOz", "CH-Lae", "US-Oho", "DE-RuW", "US-Uaf")) {
          a_YY_sub$GPP_DT_VUT_REF[a_YY_sub$GPP_DT_VUT_REF < 0] <- NA
          a_YY_sub$RECO_DT_VUT_REF[a_YY_sub$RECO_DT_VUT_REF < 0] <- NA
          sub_time_series[isub_ts, "GPP"] <- mean(a_YY_sub$GPP_DT_VUT_REF, na.rm=T)
          sub_time_series[isub_ts, c("GPPtrend_lm", "GPPp")] <- cal_sens_slope(a_YY_sub$GPP_DT_VUT_REF)
            # summary(lm(data=a_YY_sub, scale(GPP_DT_VUT_REF) ~ year_center))$coefficients[2, c(1,4)]
          sub_time_series[isub_ts, "ER"] <- mean(a_YY_sub$RECO_DT_VUT_REF, na.rm=T)
          sub_time_series[isub_ts, c("ERtrend_lm", "ERp")] <- cal_sens_slope(a_YY_sub$RECO_DT_VUT_REF)
            # summary(lm(data=a_YY_sub, scale(RECO_DT_VUT_REF) ~ year_center))$coefficients[2, c(1,4)]
          #
          # check trend outliers
          if (abs(sub_time_series[isub_ts, "GPPtrend_lm"]) > 0.25 | abs(sub_time_series[isub_ts, "ERtrend_lm"]) > 0.25) {
            plot(a_YY_sub$TIMESTAMP, a_YY_sub$GPP_DT_VUT_REF, main=isub_ts)
            plot(a_YY_sub$TIMESTAMP, a_YY_sub$NEE_VUT_REF, main=isub_ts)
            plot(a_YY_sub$TIMESTAMP, a_YY_sub$RECO_DT_VUT_REF, main=isub_ts)
          }
        } else {
          # use nighttime partitioned GPP and ER
          if (sum(!is.na(a_YY_sub$GPP_NT_VUT_REF)) >= 5) {
            a_YY_sub$GPP_NT_VUT_REF[a_YY_sub$GPP_NT_VUT_REF < 0] <- NA
            a_YY_sub$RECO_NT_VUT_REF[a_YY_sub$RECO_NT_VUT_REF < 0] <- NA
            sub_time_series[isub_ts, "GPP"] <- mean(a_YY_sub$GPP_NT_VUT_REF, na.rm=T)
            sub_time_series[isub_ts, c("GPPtrend_lm", "GPPp")] <- cal_sens_slope(a_YY_sub$GPP_NT_VUT_REF)
              # summary(lm(data=a_YY_sub, scale(GPP_NT_VUT_REF) ~ year_center))$coefficients[2, c(1,4)]
            sub_time_series[isub_ts, "ER"] <- mean(a_YY_sub$RECO_NT_VUT_REF, na.rm=T)
            sub_time_series[isub_ts, c("ERtrend_lm", "ERp")] <- cal_sens_slope(a_YY_sub$RECO_NT_VUT_REF)
              # summary(lm(data=a_YY_sub, scale(RECO_NT_VUT_REF) ~ year_center))$coefficients[2, c(1,4)]
            #
            if (abs(sub_time_series[isub_ts, "GPPtrend_lm"]) > 0.25 | abs(sub_time_series[isub_ts, "ERtrend_lm"]) > 0.25) {
              plot(a_YY_sub$TIMESTAMP, a_YY_sub$GPP_NT_VUT_REF, main=isub_ts)
              plot(a_YY_sub$TIMESTAMP, a_YY_sub$NEE_VUT_REF, main=isub_ts)
              plot(a_YY_sub$TIMESTAMP, a_YY_sub$RECO_NT_VUT_REF, main=isub_ts)
            }
          }
        }
        # scale(TIMESTAMP, scale=FALSE)
        # put the data into another data frame for second trend analysis; scale and center should be done here. 
        data_NEE_trend <- rbind(data_NEE_trend, data.frame(NEE=a_YY_sub$NEE_scale, year=a_YY_sub$year_center, 
                                                           site_ID=site_ID, IGBP=sites_long$IGBP[i], subts_ID=isub_ts))
        # 
        year_start <- years_abrupt_change[iyear_ac] + 1
        isub_ts = isub_ts + 1
      }
    }
  }
  # no abrupt change or the last section of abrupt change
  if (year_end - year_start + 1 >= threshold_nyear - 1) {
    sub_time_series[isub_ts, "site_ID"] <- site_ID
    sub_time_series[isub_ts, "year_start"] <- year_start
    sub_time_series[isub_ts, "year_end"] <- year_end
    a_YY_sub <- a_YY_good %>% filter(TIMESTAMP >= sub_time_series[isub_ts, "year_start"] & 
                                     TIMESTAMP <= sub_time_series[isub_ts, "year_end"]) %>% 
                              filter(!is.na(NEE_VUT_REF)) %>% 
                              mutate(NEE_scale = scale(NEE_VUT_REF), year_center = scale(TIMESTAMP, scale=FALSE))
    sub_time_series[isub_ts, "NEE"] <- mean(a_YY_sub$NEE_VUT_REF, na.rm=T)
    sub_time_series[isub_ts, c("NEEtrend_lm", "NEEp")] <- cal_sens_slope(a_YY_sub$NEE_scale)
      # summary(lm(data=a_YY_sub, NEE_scale ~ year_center))$coefficients[2, c(1,4)]
    #
    # use daytime GPP and ER when it is available, otherwise use other time series
    if (sum(!is.na(a_YY_sub$GPP_DT_VUT_REF)) >= 5 & !site_ID %in% c("BR-Sa1", "US-NC2", "US-SP3", "US-WCr", "US-Ho1", "US-Ho2", "US-Kon", "US-MOz", "CH-Lae", "US-NC4")) {
      a_YY_sub$GPP_DT_VUT_REF[a_YY_sub$GPP_DT_VUT_REF < 0] <- NA
      a_YY_sub$RECO_DT_VUT_REF[a_YY_sub$RECO_DT_VUT_REF < 0] <- NA 
      sub_time_series[isub_ts, "GPP"] <- mean(a_YY_sub$GPP_DT_VUT_REF, na.rm=T)
      sub_time_series[isub_ts, c("GPPtrend_lm", "GPPp")] <- cal_sens_slope(a_YY_sub$GPP_DT_VUT_REF)
        # summary(lm(data=a_YY_sub, scale(GPP_DT_VUT_REF) ~ year_center))$coefficients[2, c(1,4)]
      sub_time_series[isub_ts, "ER"] <- mean(a_YY_sub$RECO_DT_VUT_REF, na.rm=T)
      sub_time_series[isub_ts, c("ERtrend_lm", "ERp")] <- cal_sens_slope(a_YY_sub$RECO_DT_VUT_REF)
        # summary(lm(data=a_YY_sub, scale(RECO_DT_VUT_REF) ~ year_center))$coefficients[2, c(1,4)]
      # plot sites with very large trends
      if (abs(sub_time_series[isub_ts, "GPPtrend_lm"]) > 0.25 | abs(sub_time_series[isub_ts, "ERtrend_lm"]) > 0.25) {
        plot(a_YY_sub$TIMESTAMP, a_YY_sub$GPP_DT_VUT_REF, main=isub_ts)
        plot(a_YY_sub$TIMESTAMP, a_YY_sub$NEE_VUT_REF, main=isub_ts)
        plot(a_YY_sub$TIMESTAMP, a_YY_sub$RECO_DT_VUT_REF, main=isub_ts)
      }
    } else {
      if (sum(!is.na(a_YY_sub$GPP_NT_VUT_REF)) >= 5) {
        a_YY_sub$GPP_NT_VUT_REF[a_YY_sub$GPP_NT_VUT_REF < 0] <- NA
        a_YY_sub$RECO_NT_VUT_REF[a_YY_sub$RECO_NT_VUT_REF < 0] <- NA            
        sub_time_series[isub_ts, "GPP"] <- mean(a_YY_sub$GPP_NT_VUT_REF, na.rm=T)
        sub_time_series[isub_ts, c("GPPtrend_lm", "GPPp")] <- cal_sens_slope(a_YY_sub$GPP_NT_VUT_REF)
          # summary(lm(data=a_YY_sub, scale(GPP_NT_VUT_REF) ~ year_center))$coefficients[2, c(1,4)]
        sub_time_series[isub_ts, "ER"] <- mean(a_YY_sub$RECO_NT_VUT_REF, na.rm=T)
        sub_time_series[isub_ts, c("ERtrend_lm", "ERp")] <- cal_sens_slope(a_YY_sub$RECO_NT_VUT_REF)
          # summary(lm(data=a_YY_sub, scale(RECO_NT_VUT_REF) ~ year_center))$coefficients[2, c(1,4)]
        #
        if (abs(sub_time_series[isub_ts, "GPPtrend_lm"]) > 0.25 | abs(sub_time_series[isub_ts, "ERtrend_lm"]) > 0.25) {
          plot(a_YY_sub$TIMESTAMP, a_YY_sub$GPP_NT_VUT_REF, main=isub_ts)
          plot(a_YY_sub$TIMESTAMP, a_YY_sub$NEE_VUT_REF, main=isub_ts)
          plot(a_YY_sub$TIMESTAMP, a_YY_sub$RECO_NT_VUT_REF, main=isub_ts)
        }
      }
    }
    #
    # put the data into another data frame for second trend analysis; scale and center should be done here. 
    data_NEE_trend <- rbind(data_NEE_trend, data.frame(NEE=a_YY_sub$NEE_scale, year=a_YY_sub$year_center, 
                                                       site_ID=site_ID, IGBP=sites_long$IGBP[i], subts_ID=isub_ts))
    #
    isub_ts = isub_ts + 1
    ##
    # ####check the data quality
    # if (isub_ts %in% c(7,24,28,31,38,41,71,80,84,88,92,99,103,107,112,126,127,130,137,138)) {
    #   plot(a_YY_sub$TIMESTAMP, a_YY_sub$NEE_VUT_REF, main=isub_ts)
    # }
  }
}
# 
write.csv(sub_time_series, file = 'data/sub_time_series_sen.csv', row.names = FALSE)
#
```


```{r analyzing trend results, echo=TRUE}
#
# Do NEE and GPP have the same trends?
boxplot(sub_time_series$NEEtrend_lm, sub_time_series$GPPtrend_lm, sub_time_series$ERtrend_lm, xlab="NEE, GPP, ER", ylab="trends (fraction of standard deviation / year)")
# use simple t-test
t.test(sub_time_series$NEEtrend_lm, sub_time_series$GPPtrend_lm, paired=T)
# p-value = 0.01888; mean = -0.04907709
#
# NEE trend is closer to GPP trend. 
t.test(sub_time_series$NEEtrend_lm, sub_time_series$ERtrend_lm, paired=T)
# p-value = 0.1028; mean = -0.0289528 
#
# GPP and ER trends are different. 
t.test(sub_time_series$GPPtrend_lm, sub_time_series$ERtrend_lm, paired=T)
# p-value = 0.02444; mean = 0.02012429 

# correlation between NEE trend and GPP/ER trend
cor.test(sub_time_series$NEEtrend_lm, sub_time_series$GPPtrend_lm)
# cor:  -0.2163988; p-value = 0.01494
plot(sub_time_series$NEEtrend_lm, sub_time_series$GPPtrend_lm)

cor.test(sub_time_series$NEEtrend_lm, sub_time_series$ERtrend_lm)
# p-value = 0.1464
plot(sub_time_series$NEEtrend_lm, sub_time_series$ERtrend_lm)

cor.test(sub_time_series$GPPtrend_lm, sub_time_series$ERtrend_lm)
# cor:  0.7617628; p-value < 2.2e-16
plot(sub_time_series$GPPtrend_lm, sub_time_series$ERtrend_lm)
summary(lm(data=sub_time_series, GPPtrend_lm ~ ERtrend_lm))  
# this is intriguing: GPPtrend_lm = 0.025852 + 0.761793 * ERtrend_lm; Eventually ER may speed up!

################################################what if I group them into different climate classes#####################
if (!"IGBP" %in% names(sub_time_series)) {
  sub_time_series <- sub_time_series %>% left_join(sites_long[, c("site_ID", "IGBP", "Climate.class", "category")], by="site_ID")
}
####How GPP and ER vary with IGBP, climate cla
p1 <- ggplot(sub_time_series, aes(x=IGBP, y=NEEtrend_lm)) +
  geom_jitter() +
  geom_boxplot()
p1
summary(aov(data=sub_time_series, NEEtrend_lm ~ IGBP))
# not statistically different: p = 0.218; 

p2 <- ggplot(sub_time_series, aes(x=IGBP, y=GPPtrend_lm)) +
  geom_jitter() +
  geom_boxplot()
p2
summary(aov(data=sub_time_series, GPPtrend_lm ~ IGBP))
# very different: IGBP          9 0.3755 0.04172   2.299 0.0205 *

p3 <- ggplot(sub_time_series, aes(x=IGBP, y=ERtrend_lm)) +
  geom_jitter() +
  geom_boxplot()
p3
summary(aov(data=sub_time_series, ERtrend_lm ~ IGBP))
# IGBP          9 0.3951 0.04390   2.331 0.0189 *
#
cowplot::plot_grid(p1, p2, p3, nrow=1, ncol=3)
#
sub_time_series %>% ggplot(aes(x=GPPtrend_lm, y=NEEtrend_lm, col=IGBP)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  stat_cor(method = "pearson")

# strongly related to GPP trends
sub_time_series %>% filter(!IGBP %in% c('WSA', 'CSH', "MF")) %>% ggplot(aes(x=GPPtrend_lm, y=NEEtrend_lm)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  stat_cor(method = "pearson")

# not related to ER trends
sub_time_series %>% filter(!IGBP %in% c('WSA', 'CSH', "MF")) %>% ggplot(aes(x=ERtrend_lm, y=NEEtrend_lm)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  stat_cor(method = "pearson")

#
sub_time_series %>% filter(IGBP %in% c("CSH", 'WSA', "MF")) %>% ggplot(aes(x=GPPtrend_lm, y=NEEtrend_lm)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  stat_cor(method = "pearson")

# respiration trends dominated or GPP dominated?
sub_time_series %>% filter(IGBP %in% c("CSH", 'WSA', "MF")) %>% ggplot(aes(x=ERtrend_lm, y=NEEtrend_lm)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  stat_cor(method = "pearson")

#
sub_time_series %>% filter(IGBP %in% c("GRA")) %>% ggplot(aes(x=GPPtrend_lm, y=NEEtrend_lm)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  stat_cor(method = "pearson")

# be consistent with my writing.
sub_time_series %>% filter(category %in% c("dry")) %>% ggplot(aes(x=GPPtrend_lm, y=NEEtrend_lm)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  stat_cor(method = "pearson")

sub_time_series %>% filter(category %in% c("wet")) %>% ggplot(aes(x=GPPtrend_lm, y=NEEtrend_lm)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  stat_cor(method = "pearson")

sub_time_series %>% filter(category %in% c("cold") & IGBP %in% c('ENF', "EBF")) %>% ggplot(aes(x=GPPtrend_lm, y=NEEtrend_lm)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  stat_cor(method = "pearson")

sub_time_series %>% filter(category %in% c("cold") & IGBP %in% c("MF")) %>% ggplot(aes(x=GPPtrend_lm, y=NEEtrend_lm)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  stat_cor(method = "pearson")

sub_time_series %>% filter(category %in% c("cold") & IGBP %in% c("DBF")) %>% ggplot(aes(x=ERtrend_lm, y=NEEtrend_lm)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  stat_cor(method = "pearson")

sub_time_series %>% filter(category %in% c("cold") & !IGBP %in% c("DBF", "MF", 'ENF', "EBF")) %>% 
  filter(NEEtrend_lm < 0.3) %>%               # except BE-Maa
  ggplot(aes(x=GPPtrend_lm, y=NEEtrend_lm)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  stat_cor(method = "pearson")


###################what if I group them into different climate classes#####################
p4 <- ggplot(sub_time_series, aes(x=Climate.class, y=NEEtrend_lm)) +
  geom_jitter() +
  geom_boxplot()
p4
summary(aov(data=sub_time_series, NEEtrend_lm ~ Climate.class))
# p = 0.335

p5 <- ggplot(sub_time_series, aes(x=Climate.class, y=GPPtrend_lm)) +
  geom_jitter() +
  geom_boxplot()
p5
summary(aov(data=sub_time_series, GPPtrend_lm ~ Climate.class))
# p = 0.342

p6 <- ggplot(sub_time_series, aes(x=Climate.class, y=ERtrend_lm)) +
  geom_jitter() +
  geom_boxplot()
p6
summary(aov(data=sub_time_series, ERtrend_lm ~ Climate.class))
# p = 0.807

cowplot::plot_grid(p4, p5, p6, nrow=1, ncol=3)
#
sub_time_series %>% filter(!Climate.class %in% c('ET', 'Dwc', 'Dfc', "Cfb")) %>% ggplot(aes(x=GPPtrend_lm, y=NEEtrend_lm)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  stat_cor(method = "pearson")    # color=Climate.class


# It should be that some climate zones, GPP and ER are strongly corelated, but other climates may not. 

#############################################what if I group them into different DRY, WET, COLD classes#####################
p7 <- ggplot(sub_time_series, aes(x=category, y=NEEtrend_lm)) +
  geom_jitter() +
  geom_boxplot()
p7
summary(aov(data=sub_time_series, NEEtrend_lm ~ category))
# they are not statistically different. 

p8 <- ggplot(sub_time_series, aes(x=category, y=GPPtrend_lm)) +
  geom_jitter() +
  geom_boxplot()
p8

p9 <- ggplot(sub_time_series, aes(x=category, y=ERtrend_lm)) +
  geom_jitter() +
  geom_boxplot()
p9
cowplot::plot_grid(p7, p8, p9, nrow=1, ncol=3)
#
sub_time_series %>% filter(abs(GPPtrend_lm) < 0.3) %>%
ggplot(aes(x=GPPtrend_lm, y=NEEtrend_lm, color=category)) +
  geom_point() + 
  geom_smooth(method = "lm")


data <- sub_time_series %>% filter(abs(GPPtrend_lm) < 0.3) %>% filter(category=='dry')
summary(lm(data=data, NEEtrend_lm ~ GPPtrend_lm))
# (Intercept)  0.02691    0.03666   0.734   0.4743  
# GPPtrend_lm -0.59339    0.22676  -2.617   0.0194 *
  
data <- sub_time_series %>% filter(abs(GPPtrend_lm) < 0.3) %>% filter(category=='wet')
summary(lm(data=data, NEEtrend_lm ~ GPPtrend_lm))
#             Estimate Std. Error t value Pr(>|t|)   
# (Intercept)  0.02302    0.02838   0.811  0.43091   
# GPPtrend_lm -0.79941    0.24802  -3.223  0.00613 **

data <- sub_time_series %>% filter(category=='cold') %>% filter(abs(GPPtrend_lm) < 0.3) 
summary(lm(data=data, NEEtrend_lm ~ GPPtrend_lm))
# (Intercept)  0.006377   0.014137   0.451  0.65309   
# GPPtrend_lm -0.368636   0.119105  -3.095  0.00267 **

```


```{r analyzing trends in two different periods, fig.width=5, fig.height=3}
#
# add periods to sub time series; use 2011 as a threshold. 
sub_time_series_2period <- data.frame()
threshold_split_year <- 2011
for (i in 1:nrow(sub_time_series)) {
  #
  if (sub_time_series$year_end[i] <= threshold_split_year+3) {
    sub_time_series_2period <- rbind(sub_time_series_2period, 
                                     data.frame(sub_time_series[i,c(1:3,5)], period="before_2011"))
  } else if (sub_time_series$year_start[i] >= threshold_split_year-3) {
    sub_time_series_2period <- rbind(sub_time_series_2period, 
                                     data.frame(sub_time_series[i,c(1:3,5)], period="after_2011"))
  } else {
    # if time series sit between, adding to two rows
    # sub_time_series_2period <- rbind(sub_time_series_2period, 
    #                                  data.frame(sub_time_series[i,], period="before_2011"))
    # sub_time_series_2period <- rbind(sub_time_series_2period, 
    #                              data.frame(sub_time_series[i,], period="after_2011"))
    # based on middle year
    if (sub_time_series$year_end[i] + sub_time_series$year_start[i] < threshold_split_year * 2) {
      sub_time_series_2period <- rbind(sub_time_series_2period, 
                                     data.frame(sub_time_series[i,c(1:3,5)], period="before_2011"))
    } else {
      sub_time_series_2period <- rbind(sub_time_series_2period, 
                                     data.frame(sub_time_series[i,c(1:3,5)], period="after_2011"))
    }
  }
}
#
sub_time_series_2period$period <- factor(sub_time_series_2period$period, levels=c("before_2011", "after_2011"))
ggplot(sub_time_series_2period, aes(x=period, y=NEEtrend_lm)) +
  geom_jitter() +
  geom_boxplot() +
  labs(y='NEE trend')
summary(aov(data=sub_time_series_2period, NEEtrend_lm ~ period))
# p = 0.989

# for long-term sites with abrupt changes
sub_time_series_2period_paired <- sub_time_series_2period %>% filter(site_ID %in% unique(abrupt_change$site_ID))
#

sub_time_series_2period_paired_pivot <- sub_time_series_2period_paired %>% dplyr::select(c(-year_start, -year_end)) %>% pivot_wider(names_from = period, values_from = NEEtrend_lm) %>% filter(!is.na(before_2011) & !is.na(after_2011)) %>% left_join(sites_long[, c("site_ID", "IGBP", "Climate.class")], by="site_ID")
###
#
t.test(sub_time_series_2period_paired_pivot$before_2011, sub_time_series_2period_paired_pivot$after_2011, paired=T)
# t = -0.86292, df = 22, p-value = 0.3975; mean difference: -0.04098383; 
boxplot(sub_time_series_2period_paired_pivot$before_2011, sub_time_series_2period_paired_pivot$after_2011, xlab='before 2011, after 2011', ylab='NEE trend')

### focusing on cold climate, NEE trend increased a lot (carbon sequestration reduced). 
sub_time_series_2period_paired_pivot_climate <- sub_time_series_2period_paired_pivot %>% filter(Climate.class %in% c("Dfb", "Dfc", "Dwc", "Dfa", "Cfb"))
t.test(sub_time_series_2period_paired_pivot_climate$before_2011, sub_time_series_2period_paired_pivot_climate$after_2011, paired=T)
# t = -2.359, df = 17, p-value = 0.03055
# mean difference 
#       -0.08680022

# focus on forests, trend is not clear. 
sub_time_series_2period_paired_pivot_veg <- sub_time_series_2period_paired_pivot %>% filter(IGBP %in% c("DBF", "ENF", "MF"))
t.test(sub_time_series_2period_paired_pivot_veg$before_2011, sub_time_series_2period_paired_pivot_veg$after_2011, paired=T)
# p-value = 0.0702

### all sites with abrupt changes
sub_time_series_2period_paired %>% ggplot(aes(x=period, y=NEEtrend_lm)) +
  geom_jitter() +
  geom_boxplot()
#
### the site in cold climate only; in cold climate NEEtrend declined. 
data_plot <- sub_time_series_2period_paired_pivot_climate %>% pivot_longer(cols=2:3, names_to='period', values_to='NEEtrend') 
data_plot$period <- factor(data_plot$period, levels=c("before_2011", "after_2011"))
  ggplot(data=data_plot, aes(x=period, y=NEEtrend)) +
  geom_jitter() +
  geom_boxplot()


########does the trend vary significantly among vegetation, climate classes? #######
sub_time_series_anova <- sub_time_series
summary(aov(data=sub_time_series_anova, NEEtrend_lm ~ Climate.class))    # p = 0.191
summary(aov(data=sub_time_series_anova, NEEtrend_lm ~ IGBP))        # p = 0.189
summary(aov(data=sub_time_series_anova, NEEtrend_lm ~ category))    # p = 0.563
#
ggplot(data=sub_time_series_anova, aes(x=IGBP, y=NEEtrend_lm)) +
  geom_boxplot() +
  geom_jitter()
#
sub_time_series_anova <- sub_time_series_anova %>% 
                        mutate(vegetation = case_when(IGBP %in% c("ENF", "DBF", "MF", "EBF", "OSH", "CSH") ~ "forest & shrub", 
                                                    IGBP %in% c("GRA", "WSA") ~ "grassland & woody savanah", 
                                                    IGBP %in% c("WET") ~ "wetland", 
                                                    IGBP %in% c("SAV") ~ "savanah"))
                                                 #   IGBP %in% c("OSH", "CSH") ~ "shrubland", 
                                                 #   IGBP %in% c("WSA", "SAV") ~ "savanah"))
summary(aov(data=sub_time_series_anova, NEEtrend_lm ~ vegetation))
#              Df Sum Sq Mean Sq F value Pr(>F)  
# vegetation    3 0.2511 0.08368   3.863 0.0111 *
# Residuals   123 2.6643 0.02166 

ggplot(data=sub_time_series_anova, aes(x=vegetation, y=NEEtrend_lm)) +
  geom_boxplot() +
  geom_jitter()
#
# Grasslands have positive trends. 
#
# Next step is to calculate overall trend, considering spatial structure and vegetation class [try the new package or Rstan]. 
# I need to spend some time on this. 
# Bayesian method to calcualte this value. 

# US-KS2 was removed because of shorter time series after removing some years. 


# what if the site with detected trend change?

# It is better to compare NEE trends with GPP and ER trends

# How about comparing this with fertilization effects from manipulative experiments?


```


```{r normalize values}
norm_range <- function(x){
  (x-min(x, na.rm = T))/(max(x, na.rm = T)-min(x, na.rm = T))
}
```


# The method I want to try: "Bayesian Integrated Nested Laplace Approximation"
```{r INLA to calculate overall trends}
library(INLA)
library(geosphere)
#
# add period into this df
sub_time_series_2period$subts_ID <- as.numeric(rownames(sub_time_series_2period))
#
#
data_NEE_trend <- data_NEE_trend %>% left_join(sub_time_series_2period[, c("subts_ID", "period")], by="subts_ID")
#
# mutate site_code, IGBP_code, and subts_code
data_NEE_trend <- data_NEE_trend %>% mutate(site_code = as.numeric(as.factor(site_ID)), 
                                            site_code2 = as.numeric(as.factor(site_ID)), 
                                            IGBP_code = as.numeric(as.factor(IGBP)), 
                                            period_code = as.numeric(as.factor(period)))
# 
data_NEE_trend <- data_NEE_trend %>% left_join(sites_long[, c("site_ID", "Lat", "Long")]) 
# 
coordinates <- unique(data_NEE_trend[, c("site_code", "Long", "Lat")]) %>% arrange(site_code)
#
spa_mat_trim <- as.matrix(geosphere::distm(coordinates[,2:3], fun = distHaversine))/1000000
# normalize
spa_mat_trim = norm_range(spa_mat_trim)
spa_mat_trim = abs(spa_mat_trim - 1)
spa_mat_trim = ifelse(spa_mat_trim == 0, sort(unique(as.vector(spa_mat_trim)))[2], spa_mat_trim)
colnames(spa_mat_trim) = coordinates$site_code
rownames(spa_mat_trim) = coordinates$site_code
spa_mat_trim = solve(spa_mat_trim)
#
# write the first INLA model
# the same prior is used for all

prior_prec = "expression:
  log_dens = 0 - log(2) - theta / 2;
  return(log_dens);
"
#
m1 = inla(NEE ~ year +
              f(subts_ID, year, model = "iid", 
                constr = F, hyper = prior_prec) +
              f(period_code, year, model = "iid", 
                constr = F, hyper = prior_prec) + 
              f(site_code2, year, model = "generic0",
                constr = F, Cmatrix = spa_mat_trim, hyper = prior_prec) + # We specify that sites should co-vary according to a spatial covariance matrix (Cmatrix argument).
              # f(site_code, year, model = "iid",
              #   constr = F, hyper = prior_prec) +
              f(IGBP_code, year, model = "iid", 
                constr = F, hyper = prior_prec),
              data = data_NEE_trend, family = "gaussian", 
              control.predictor=list(compute=TRUE),
              control.fixed = list(mean.intercept = 0, prec.intercept = 0.001,
                                   mean = 0, prec = 1),
              num.threads = 4)
#
# extract the NEE trend result for each sub time series. 
get_NEE_trend <- distinct(data_NEE_trend[, c('subts_ID', 'site_code', 'site_code2', 'IGBP_code', 'period_code')])
#
# overall trend for all sub time series
get_NEE_trend$trend_all <- m1$summary.fixed$mean[2]
#
# trend for individual sub time series
trend_random <- data.frame(subts_ID=m1$summary.random$subts_ID[,1], 
                           trend_subts=m1$summary.random$subts_ID[,2])
get_NEE_trend <- merge(get_NEE_trend, trend_random, by='subts_ID', all.x = TRUE)
# trend for individual period
trend_random <- data.frame(period_code=m1$summary.random$period_code[,1], 
                           trend_period=m1$summary.random$period_code[,2])
get_NEE_trend <- merge(get_NEE_trend, trend_random, by='period_code', all.x = TRUE)
# trend for individual site
trend_random <- data.frame(site_code2=m1$summary.random$site_code2[,1], 
                           trend_site2=m1$summary.random$site_code2[,2])
get_NEE_trend <- merge(get_NEE_trend, trend_random, by='site_code2', all.x = TRUE)
# trend for individual IGBP
trend_random <- data.frame(IGBP_code=m1$summary.random$IGBP_code[,1], 
                           trend_IGBP=m1$summary.random$IGBP_code[,2])
get_NEE_trend <- merge(get_NEE_trend, trend_random, by='IGBP_code', all.x = TRUE)
get_NEE_trend$trend_inla <- rowSums(get_NEE_trend[, c("trend_all", "trend_subts", "trend_period", "trend_site2", "trend_IGBP")])
trend_random <- sub_time_series %>% mutate(subts_ID=as.numeric(rownames(sub_time_series))) %>%
                    merge(get_NEE_trend[, c('subts_ID', 'trend_inla')], by='subts_ID',all.x = TRUE)
# the two trends are highly correlated but inla values does not change as much as linear regression for large trends. 
trend_random %>% na.omit %>% ggplot(aes(x=NEEtrend_lm, y=trend_inla)) + 
  geom_point()
##
#####Eventually, I need a function to calculate individual trends. 
# what if I only work on the sites with abrupt changes
data_NEE_trend_ac <- data_NEE_trend %>% subset(site_ID %in% unique(abrupt_change$site_ID)) 

# across all the time series, there is no difference in NEE trend between before 2011  

```

