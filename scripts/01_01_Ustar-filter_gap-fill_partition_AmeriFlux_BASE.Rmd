---
title: "gapfill_partition_for_AmeriFlux_BASE"
output: pdf_document
date: "2025-01-09"
author: "Junna Wang"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```


```{r raw data location}
rm(list=ls())
# change this location where the raw data will be saved accordingly
# dir_rawdata <- '/Users/jw2946/Documents/stability_project/SiteData/'
dir_rawdata <- '/Volumes/MaloneLab/Research/Stability_Project/SiteData'
#
```


```{r which sites I need to gap-fill}
####
library(librarian)
shelf(ggplot2, dplyr, tidyr, lubridate)
####
#
# find the sites with >= 10 year data, and non crop sites
# this following meta file is downloaded from Ameriflux with the search criteria: FC=true, NEE=true, data length >= 5 years
# sites <- read.csv('data/AmeriFlux-site-search-results-202501071742.csv')
# we used FC=true, length >=10 years
sites <- read.csv('data/AmeriFlux-site-search-results-202512092232.csv')
#
# remove cropland "CRO", Cropland/natural vegetation mosaics "CVM", "BSV": Barren?, water: "WAT", Urban and built-up lands:"URB", Snow and ice: "SNO"
sites_selected <- sites %>% filter(!Vegetation.Abbreviation..IGBP. %in% c("CRO", "CVM", "BSV", "WAT", "URB", "SNO")) %>% filter(Data.Use.Policy=='CC-BY-4.0')
## 113 sites
##
## US-Ho3 and CA-SJ2 were removed because they are harvested
## US-Ho3: Howland Forest (harvest site); shelterwood harvest clearly affect NEE (https://www.mdpi.com/2073-445X/10/4/436); 
## CA-SJ2: Saskatchewan - Western Boreal, Jack Pine forest harvested in 2002; removed because they are a series of sites. 
## CA-SJ1: Saskatchewan - Western Boreal, Jack Pine forest harvested in 1994; measurement from 2000.
## CA-Qcu: Quebec - Eastern Boreal, Black Spruce/Jack Pine Cutover; removed due to cutover
## US-An1 and US-An2: burned, moderately burned sites in 2007.removed
## US-SP2: plantted in Jan. 1999; removed!
## US-UiD: Mesuarement paused in 2016~2024, removed. 
## "US-KL2", "US-KL3", US-KM2, US-KM3: should use data after 2011, due to consistent measurement
## US-KM4: should use data after 2010
## US-Fo1: data paused for a while, only measured during campaigns
sites_selected_BASE <- sites_selected %>% filter(AmeriFlux.BASE.Data.End - AmeriFlux.BASE.Data.Start >= 9) %>% filter(AmeriFlux.FLUXNET.Data == "No") %>% filter(!Site.ID %in% c("US-Ho3", "CA-SJ2", "CA-SJ1", "US-An1", "US-An2", "US-SP2", "US-UiD", "CA-Qcu", "US-Fo1"))
##
sites_selected_BASE <- rbind(sites_selected_BASE, sites_selected[sites_selected$Site.ID == 'CA-Cbo', ])
## sites I should work on: "BR-Ma2" "US-An3" "US-Aud" "US-Blo" "US-ChR" "US-Cwt" "US-Ha2" "US-IB2" "US-KL2" "US-KL3"
## "US-KM2" "US-KM3" "US-KM4" "US-LA2" "US-MBP" "US-Me4" "US-NR3" "US-NR4" "US-Vcs" "US-Wrc"


```


```{r a function calculate effective data duration}
#
# Function to calculate lengths of NA gaps
na_gap_lengths <- function(vec) {
  rle(is.na(vec))$lengths[which(rle(is.na(vec))$values)]
}

# this function calculates data duration, excluding long gaps (unit is days)
data_duration_day <- function(dt, gap_thresh, time_series) {
  #
  id_start <- which(!is.na( time_series ))[1]
  id_end   <- tail(which(!is.na( time_series )), 1)
  # difftime return days of the difference
  data_duration_day <- (id_end - id_start + 1) * dt
#  print(data_duration_day)
  # substract the gap times longer than half year
  gap_lengths <- na_gap_lengths(time_series [id_start:id_end])
  # convert gap_lengths to gap time
  gap_times <- gap_lengths * dt
  if (any(gap_times > gap_thresh)) {
    gap_times_half_year <- sum(gap_times[gap_times > gap_thresh])
    data_duration_day <- data_duration_day - gap_times_half_year
  }
  return(data_duration_day)
}

# # testing this function
# data_duration_day(0.5/24, 182, data_BASE$FC_1_1_1)
# plot(data_BASE$FC_1_1_1)

```


```{r remove sites with limited FC data}
####
library(librarian)
shelf(amerifluxr)
#
#
FC_duration_thresh <- 10 * 365       # unit days
gap_thresh <- 182                   # unit days
#
site_to_remove <- c()
#
# location to save Ameriflux Base data
files <- list.files(file.path(dir_rawdata, 'Ameriflux_BASE'), pattern="zip", full.names = T)
#
for (i in 1:nrow(sites_selected_BASE)) {
#  i = 8
  name_site <- sites_selected_BASE$Site.ID[i]
  print(name_site)
  #
  # read Ameriflux base files
  data_BASE <- amf_read_base(files[grepl(name_site, files)], parse_timestamp=TRUE, unzip = T)
  #  
  var_names <- names(data_BASE)
  #
  FC_name <- var_names[grepl('^FC', var_names)]
  #
  dt <- as.numeric(difftime(data_BASE$TIMESTAMP[2], data_BASE$TIMESTAMP[1], units=c('days')))
  FC_duration <- data_duration_day(dt, gap_thresh, data_BASE[,FC_name[1]])
  #
  # print(plot(data_BASE[,FC_name[1]]))
  # 
  if (FC_duration < FC_duration_thresh) {
    site_to_remove <- c(site_to_remove, name_site)
  }
  print(paste0(i, '  ', FC_duration))
  #
}
#
# sites to remove: "US-An3" "US-Blo" "US-Ced" "US-LA2" "US-Me4" "US-Vcs" (large gaps year after year)
# this code will take ~10 min

```


```{r a function to merge ERA5 and FLUX}

blend_ERA5_FLUX <- function(merged_data, varname_FLUX, varname_ERA5, blending_rule) {
  for (i in seq_along(varname_FLUX)) {
    flux_var <- varname_FLUX[i]
    era5_var <- varname_ERA5[i]
    rule <- blending_rule[i]  # Extract corresponding rule for the variable
    
    message("Processing: ", flux_var, " using rule: ", rule)
    
    # Initialize the new "_f" column with the original flux_var values
    merged_data[[paste0(flux_var, "_f")]] <- merged_data[[flux_var]]
    
    if (rule == "replace") {
      # Replace the whole flux variable with ERA5 (copy ERA5 values to new column)
      merged_data[[paste0(flux_var, "_f")]] <- merged_data[[era5_var]]
    }
    
    if (rule == "lm") {
      # Remove rows with NA in either varname_FLUX or varname_ERA5
      complete_cases <- merged_data[!is.na(merged_data[[flux_var]]) & !is.na(merged_data[[era5_var]]), ]
      
      if (nrow(complete_cases) > 1) {  
        formula_str <- paste(flux_var, "~", era5_var)
        message("Fitting linear model with formula: ", formula_str)
        
        lm_model <- lm(as.formula(formula_str), data = complete_cases)
        
        # Only use model predictions to fill missing values in flux_var
        missing_flux_idx <- which(is.na(merged_data[[flux_var]]))  # Identify all missing flux_var entries
        
        if (length(missing_flux_idx) > 0) {
          # Predict missing flux values based on ERA5 data and fill them
          predictions <- predict(lm_model, 
                                 newdata = setNames(data.frame(merged_data[[era5_var]][missing_flux_idx]), era5_var))
          
          # Ensure the predictions fill only the missing values in the new "_f" column
          merged_data[[paste0(flux_var, "_f")]][missing_flux_idx] <- predictions
        }
      }
    }
    
    if (rule == "lm_no_intercept") {
      complete_cases <- merged_data[!is.na(merged_data[[flux_var]]) & !is.na(merged_data[[era5_var]]), ]
      
      if (nrow(complete_cases) > 1) {  
        formula_str <- paste(flux_var, "~", era5_var, "+ 0")  # Linear model without intercept
        message("Fitting linear model without intercept with formula: ", formula_str)
        
        lm_model_no_intercept <- lm(as.formula(formula_str), data = complete_cases)
        
        # Only use model predictions to fill missing values in flux_var
        missing_flux_idx <- which(is.na(merged_data[[flux_var]]))  # Identify all missing flux_var entries
        
        if (length(missing_flux_idx) > 0) {
          # Predict missing flux values based on ERA5 data and fill them
          predictions <- predict(lm_model_no_intercept, 
                                 newdata = setNames(data.frame(merged_data[[era5_var]][missing_flux_idx]), era5_var))
          
          # Ensure the predictions fill only the missing values in the new "_f" column
          merged_data[[paste0(flux_var, "_f")]][missing_flux_idx] <- predictions
        }
      }
    }
    
    if (rule == "automatic") {
      total_count <- nrow(merged_data)
      non_na_count <- sum(!is.na(merged_data[[flux_var]]))
      
      if ((non_na_count / total_count) >= 0.5) {
        # Remove rows with NA in either varname_FLUX or varname_ERA5
        complete_cases <- merged_data[!is.na(merged_data[[flux_var]]) & !is.na(merged_data[[era5_var]]), ]
        
        if (nrow(complete_cases) > 1) {  
          formula_str <- paste(flux_var, "~", era5_var)
          message("Fitting linear model with formula: ", formula_str)
          
          lm_model <- lm(as.formula(formula_str), data = complete_cases)
          
          # Only use model predictions to fill missing values in flux_var
          missing_flux_idx <- which(is.na(merged_data[[flux_var]]))  # Identify all missing flux_var entries
          
          if (length(missing_flux_idx) > 0) {
            # Predict missing flux values based on ERA5 data and fill them
            predictions <- predict(lm_model, 
                                   newdata = setNames(data.frame(merged_data[[era5_var]][missing_flux_idx]), era5_var))
            
            # Ensure the predictions fill only the missing values in the new "_f" column
            merged_data[[paste0(flux_var, "_f")]][missing_flux_idx] <- predictions
          }
        }
      } else {
        message("More than 50% missing in ", flux_var, ", replacing with ", era5_var)
        # If more than 50% of FLUX data is missing, replace the entire FLUX variable with ERA5
        merged_data[[paste0(flux_var, "_f")]] <- merged_data[[era5_var]]
      }
    }
  }
  
  return(merged_data)
}

```


```{r map each variable and their priority}
library(librarian)
shelf(amerifluxr)
#
# remove 6 sites that are not long enough, using results from the last chunk. 
# site_to_remove <- c("US-An3", "US-Blo", "US-Ced", "US-LA2", "US-Me4", "US-Vcs")       # this is actually calculated in the previous chunks
# further remove "US-xSP", because of poor data quality
site_to_remove <- c(site_to_remove, "US-xSP")
#
sites_selected_BASE1 <- sites_selected_BASE %>% filter(!Site.ID %in% c(site_to_remove))

#
NEE_names_all   <- c( )         # NEE_names_order <- c( )
LE_names_all    <- c( )
H_names_all     <- c( )         # H (sensitive heatflux)
PPFD_IN_names_all  <- c( )      # PPFD and SW_IN can be replaced with each other. 
SW_IN_names_all <- c( )
TA_names_all    <- c( )
RH_names_all    <- c( )         # RH and VPD can be replaced with each other; relative humidity   
VPD_names_all   <- c( )
NETRAD_names_all  <- c( )       # (net radiation)
USTAR_names_all   <- c( )
CO2_names_all     <- c( )
TS_names_all      <- c( )       # TS soil temperature
SWC_names_all     <- c( )
#
## FC and SC are added later for some sites NEE is not calculated well.
FC_names_all <- c( )
SC_names_all <- c( )
#
# location to save Ameriflux Base data
files <- list.files(file.path(dir_rawdata, 'Ameriflux_BASE'), pattern="zip", full.names = T)
#
for (i in 1:nrow(sites_selected_BASE1)) {
#  i = 1
  name_site <- sites_selected_BASE$Site.ID[i]
  print(name_site)
  #
  # read Ameriflux base files
  data_BASE <- amf_read_base(files[grepl(name_site, files)], parse_timestamp=TRUE, unzip = T)
  #
  # collect variable names and converted to uppercase
  var_names <- toupper(names(data_BASE))
  #
  # collect relevant column names
  NEE_names_all      <- c(NEE_names_all,     var_names[grepl('^NEE', var_names)])
  LE_names_all       <- c(LE_names_all,      var_names[grepl('^LE',  var_names)])
  # need to process differently for H
  H_names_all        <- c(H_names_all,       var_names['H' == var_names | grepl('^H_', var_names)])
  TA_names_all       <- c(TA_names_all,      var_names['TA' == var_names | grepl('^TA_',  var_names) | 'T_SONIC' == var_names ])
  PPFD_IN_names_all  <- c(PPFD_IN_names_all, var_names[grepl('^PPFD_IN',  var_names)])
  SW_IN_names_all    <- c(SW_IN_names_all,   var_names[grepl('^SW_IN',  var_names)])
  #
  RH_names_all       <- c(RH_names_all,      var_names[grepl('^RH',  var_names)])
  VPD_names_all      <- c(VPD_names_all,     var_names[grepl('^VPD',  var_names)])
  NETRAD_names_all   <- c(NETRAD_names_all,  var_names[grepl('^NETRAD',  var_names)])
  USTAR_names_all    <- c(USTAR_names_all,   var_names[grepl('^USTAR',  var_names)])
  CO2_names_all      <- c(CO2_names_all,     var_names[grepl('^CO2',  var_names)])  
  # TS and SWC
  TS_names_all       <- c(TS_names_all,      var_names['TS' == var_names | grepl('^TS_',  var_names)])
  SWC_names_all      <- c(SWC_names_all,     var_names[grepl('^SWC',  var_names)])
  #
  FC_names_all       <- c(FC_names_all,      var_names[grepl('^FC',  var_names)])
  SC_names_all       <- c(SC_names_all,      var_names[grepl('^SC',  var_names)])
}
#
#######################################################################################################################
# write a function to give priority to these variable names
sort_var_names <- function(var_name, var_names) {
  #
  var_names <- unique(var_names)
  sort_var_names <- c()
  # the order of priority
  # single name > single name with with PI > single name with with PI_F
  # 
  pattern        <- c(var_name, paste0(var_name, "_PI"), paste0(var_name, "_PI_F"))
  #
  # layers data to use: CO2 data use the top 4 layers, but other variables use the top 2 layers only
  if (var_name=='CO2') {
    layer_id <- 1:4
  } else {
    layer_id <- 1:2
  }
  #
  for (i in 1:length(pattern)) {
    sort_var_names_group <- c()
    pattern_add    <- paste0("\\b", pattern[i], "(?:_\\d.*)?\\b")
    if (any(grepl(pattern_add, var_names))) {
      var_names_pattern <- var_names[grepl(pattern_add, var_names)]
      # Priority 1: variable name == pattern
      if (pattern[i] %in% var_names) {
        sort_var_names_group <- c(sort_var_names_group, pattern[i])
      }
      ##############
      # Priority 2: use data of the top two layers of observation.
      # Check if there is numbers in variable names
      if (any(grepl("[0-9]", var_names_pattern))) {
        #
        # get numbers in these column names: only use data of the top two layers of observation. 
        for (j in 1:length(var_names_pattern)) {
          var_name_split   <- strsplit(var_names_pattern[j], split = "_")
          var_name_2number <- suppressWarnings(as.numeric(var_name_split[[1]]))
          var_name_number  <- var_name_2number[!is.na(var_name_2number)]
          # the variable name has to contain 3 numbers and the middle number is in layer_id
          if (length(var_name_number) == 3) {
            if (var_name_number[2] %in% layer_id) {
              sort_var_names_group <- c(sort_var_names_group, var_names_pattern[j])
            }
          }
          # Alternatively, the variable name has to contain 1 number and the number is in layer_id: eg: CA-Oas;  
          if (length(var_name_number) == 1) {
            if (var_name_number[1] %in% layer_id) {
              sort_var_names_group <- c(sort_var_names_group, var_names_pattern[j])
            }
          }
        }   ## finish the loop
      }  ## finish the Priority 2
      sort_var_names <- c(sort_var_names, sort(sort_var_names_group))
    }  ## finish each pattern
  }  ## finish all patterns
  ##
  ## deal with the air temperature problem, if some sites have T_SONIC, adding this variable
  if (var_name == "TA") {
    if ( 'T_SONIC' %in% var_names ) {
      sort_var_names <- c(sort_var_names, 'T_SONIC')
    }
  }
  return(sort_var_names)
}  ## finish the function
#
#################################################################################################################
# use the function
NEE_names_sort     <- sort_var_names("NEE",    NEE_names_all)
LE_names_sort      <- sort_var_names("LE",     LE_names_all)
H_names_sort       <- sort_var_names("H",      H_names_all)
PPFD_IN_names_sort <- sort_var_names("PPFD_IN",   PPFD_IN_names_all)
SW_IN_names_sort   <- sort_var_names("SW_IN",  SW_IN_names_all)
TA_names_sort      <- sort_var_names("TA",     TA_names_all)
RH_names_sort      <- sort_var_names("RH",     RH_names_all)
VPD_names_sort     <- sort_var_names("VPD",    VPD_names_all)
NETRAD_names_sort  <- sort_var_names("NETRAD", NETRAD_names_all)
USTAR_names_sort   <- sort_var_names("USTAR",  USTAR_names_all)
CO2_names_sort     <- sort_var_names("CO2",    CO2_names_all)
TS_names_sort      <- sort_var_names("TS",     TS_names_all)
SWC_names_sort     <- sort_var_names("SWC",    SWC_names_all)
##
#### NEE should be deal with specifically, if NEE values is not provided, but my sites all include NEE.
FC_names_sort <- c("FC", "FC_1_1_1")  # 
SC_names_sort <- c("SC", "SC_1_1_1", "SC_1_1_2")
NEE_names_sort <- c("NEE", NEE_names_sort)         # add NEE because I will make a new NEE column.
##
```


```{r Ustar-filter, gap-fill, and partition}
## it will take ~ 8 hours to finish running this chunk. 
## A part of the code of this section is from Ammara Talib, 1/9/2025
## use Ustar threshold for each year to be consistent with FLUXNET protocols
library(librarian)
shelf(Rcpp, REddyProc, amerifluxr, tidyverse, lutz)
#
#https://rdrr.io/cran/flux/man/gpp.html
#https://rdrr.io/rforge/REddyProc/man/sEddyProc.example.html
#
# location to save Ameriflux Base data
files     <- list.files(file.path(dir_rawdata, 'Ameriflux_BASE'), pattern="zip", full.names = T)
files_ERA <- list.files(file.path(dir_rawdata, 'ERA5_Tair_Prec'), pattern=".csv$", full.names = T)
files_ERA_SW <- list.files(file.path(dir_rawdata, 'ERA5_SW'), pattern=".csv$", full.names = T)
for (i in 1:nrow(sites_selected_BASE1)) {
  # "BR-Ma2" "BR-Sa1" "CA-Cbo" "US-Aud" "US-ChR" "US-Cwt" "US-Ha2" "US-IB2" "US-KL2" "US-KL3" "US-KM2" "US-KM3" "US-NC2"
  # "US-KM4" "US-MBP" "US-NR3" "US-NR4"
  # i = 2
  ####Step 0: Prepare for requested variables for each site: DateTime, NEE, Rg (shortwave radiation), Tair, Ustar, rH (relative humidity), and VPD
  
  name_site <- sites_selected_BASE1$Site.ID[i]
  print(name_site)
  if (!name_site %in% c("BR-Sa1", "CA-Cbo")) {
    next
  }
  
  # the site US-PFa should be processed separately, so skip it now. 
  if (name_site == "US-PFa") { next }
  #
  # read Ameriflux base files
  data_BASE <- amf_read_base(files[grepl(name_site, files)], parse_timestamp=TRUE, unzip = T)
  # set abnormal values (<=-9999) to NA
  data_BASE[data_BASE<=-9999] <- NA
  
  if (name_site == "US-KM2") {
    data_BASE$TS_1_2_1 <- as.numeric(data_BASE$TS_1_2_1)
  }
  #
  dt <- as.numeric(difftime(ymd_hm(data_BASE$TIMESTAMP_END[2]), ymd_hm(data_BASE$TIMESTAMP_END[1]), units=c('days')))
  #
  # remove the low quality data for US-Elm and US-Esm due to the shift of time after 2016
  # remove the low quality data for US-NC2 due to the incomplete and abnormal NEE values after 2021
  if (name_site %in% c('US-Elm', 'US-Esm')) {
    data_BASE <- data_BASE %>% filter(year(ymd_hm(data_BASE$TIMESTAMP_START)) <= 2016)
  } else if (name_site %in% c('US-NC2')) {
    data_BASE <- data_BASE %>% filter(year(ymd_hm(data_BASE$TIMESTAMP_START)) <= 2021)
  }
  #
  var_names <- names(data_BASE)
  # 
  # check if this is forest site
  IGBP <- sites_selected_BASE1$Vegetation.Abbreviation..IGBP.[i]
  if (IGBP %in% c("DBF", "DNF", "EBF", "ENF", "MF")) {
    is_forest = TRUE
  } else {
    is_forest = FALSE
  }
  #
  # calculate NEE, and adding one more column
  # note: the original design is forest sites have to have FC and SC, but we later allowed forest sites to have FC only. 
  if (is_forest) {
    if (any(FC_names_sort %in% var_names) & any(SC_names_sort %in% var_names)) {
      data_BASE$NEE <- coalesce(!!!data_BASE[, FC_names_sort[FC_names_sort %in% var_names], drop=FALSE])  
      SC <- coalesce(!!!data_BASE[, SC_names_sort[SC_names_sort %in% var_names], drop=FALSE])
      data_BASE$NEE[!is.na(data_BASE$NEE) & !is.na(SC)] <- data_BASE$NEE[!is.na(data_BASE$NEE) & !is.na(SC)] + SC[!is.na(data_BASE$NEE) & !is.na(SC)] 
    } else if (any(FC_names_sort %in% var_names)) {
      data_BASE$NEE <- coalesce(!!!data_BASE[, FC_names_sort[FC_names_sort %in% var_names], drop=FALSE])       
    }
  } else {
    if ( any(FC_names_sort %in% var_names) ) {
      data_BASE$NEE <- coalesce(!!!data_BASE[, FC_names_sort[FC_names_sort %in% var_names], drop=FALSE])
      #
      if ( any(SC_names_sort %in% var_names) ) {
        SC <- coalesce(!!!data_BASE[, SC_names_sort[SC_names_sort %in% var_names], drop=FALSE])
        data_BASE$NEE[!is.na(data_BASE$NEE) & !is.na(SC)] <- data_BASE$NEE[!is.na(data_BASE$NEE) & !is.na(SC)] + SC[!is.na(data_BASE$NEE) & !is.na(SC)]
      }
    } 
  }
  plot(data_BASE$NEE)
  # update variable names because I add a new column.
  var_names <- names(data_BASE)
  #
  #  12 variables are selected.  
  key_name_list      <- list()
  key_name_list[[1]] <- NEE_names_sort[NEE_names_sort %in% var_names]
  key_name_list[[2]] <- LE_names_sort[LE_names_sort %in% var_names]
  key_name_list[[3]] <- H_names_sort[H_names_sort %in% var_names]
  #  only choose one from PPFD_IN and SW_IN; give priority to SW_IN?
  PPFD_IN_used <- TRUE
  if ( any(PPFD_IN_names_sort %in% var_names) ) {
    key_name_list[[4]] <- PPFD_IN_names_sort[PPFD_IN_names_sort %in% var_names]
  } else {
    key_name_list[[4]] <- SW_IN_names_sort[SW_IN_names_sort %in% var_names]
    PPFD_IN_used <- FALSE
  }
  key_name_list[[5]] <- TA_names_sort[TA_names_sort %in% var_names]
  key_name_list[[6]] <- RH_names_sort[RH_names_sort %in% var_names]
  key_name_list[[7]] <- VPD_names_sort[VPD_names_sort %in% var_names]
  key_name_list[[8]] <- NETRAD_names_sort[NETRAD_names_sort %in% var_names]
  key_name_list[[9]] <- USTAR_names_sort[USTAR_names_sort %in% var_names]
  key_name_list[[10]] <- CO2_names_sort[CO2_names_sort %in% var_names]
  key_name_list[[11]] <- TS_names_sort[TS_names_sort %in% var_names]
  key_name_list[[12]] <- SWC_names_sort[SWC_names_sort %in% var_names]
  #
  # prepare data for ReddyProc
  data_EddyProc <- data.frame(DateTime = ymd_hm(data_BASE$TIMESTAMP_START))
  col_names <- c("NEE", "LE", "H", "Rg", "Tair", "RH", "VPD", "NETRAD", "Ustar", "CO2", "TS", "SWC")
  col_names_used <- rep(FALSE, 12)
  for (j in 1:length(key_name_list)) {
    if ( length(key_name_list[[j]]) > 0 ) {
      # to ensure data existence in these columns
      if (sum(!is.na(data_BASE[, key_name_list[[j]]])) > 0) {
        new_col <- coalesce(!!!data_BASE[, key_name_list[[j]], drop=FALSE])
        data_EddyProc <- cbind(data_EddyProc, new_col)
        print(key_name_list[[j]])
        print(plot(data_EddyProc[,1], new_col, main=col_names[j]))
        col_names_used[j] = TRUE
      }
    }
  }
  names(data_EddyProc) <- c("DateTime", col_names[col_names_used])
  if (!PPFD_IN_used) {
    data_EddyProc$Rg <- data_EddyProc$Rg * 2.3      # use the PPFD_IN unit. 
  }
  #
  # check if VPD exists or if it is complete; if not, recalculate it using RH
  if (!col_names_used[7] | sum(!is.na(data_EddyProc$VPD)) + 365 * 24 < sum(!is.na(data_EddyProc$RH))) {
    data_EddyProc$VPD <- fCalcVPDfromRHandTair(data_EddyProc$RH, data_EddyProc$Tair)
    col_names_used[7] <- TRUE
  }
  #  set reasonable range of Rg, NEE values. 
  data_EddyProc$Rg <- pmax(data_EddyProc$Rg, 0.0)
  data_EddyProc$NEE[abs(data_EddyProc$NEE) > 40] <- NA
  #
  #  remove some abnormal fluxes during very low temperature
  if (name_site == 'CA-Man') {
    data_EddyProc$NEE[abs(data_EddyProc$NEE) > 1 & data_EddyProc$Tair < -5] <- NA
  }
  #
  # cut off the first few years without NEE/Ustar data; required, otherwise it does not gap fill.  
  year_first_NEE <- year(data_EddyProc$DateTime[which(!is.na(data_EddyProc$Ustar))[1]])
  data_EddyProc <- data_EddyProc %>% filter(year(DateTime) >= year_first_NEE)
  
  # using ERA5 data to gap-fill Rg through linear regression without intercept
  if (any(grepl(paste0(name_site, '_ssrd_'), files_ERA_SW))) {
    df_sw <- read.csv(files_ERA_SW[grepl(paste0(name_site, '_ssrd_'), files_ERA_SW)])
    df_sw$time <- ymd_hm(df_sw$time)
    df_sw <- data_EddyProc %>% select('DateTime', 'Rg') %>% left_join(df_sw, by=c("DateTime" = "time"))
    df_sw <- blend_ERA5_FLUX(merged_data=df_sw, varname_FLUX='Rg', varname_ERA5='SW_ERA', blending_rule="lm_no_intercept")
    data_EddyProc$Rg <- df_sw$Rg_f    
  }
  
  # initiate an EddyProc object
  EProc    <- sEddyProc$new(name_site, data_EddyProc, col_names[col_names_used], DTS=1/dt)
  ##################################################
  # Partitioning
  lat_site  <- sites_selected_BASE1$Latitude..degrees.[1]
  long_site <- sites_selected_BASE1$Longitude..degrees.[1]
  tz <- tz_offset(as.Date("2000-01-01"), tz_lookup_coords(lat=lat_site, lon=long_site, method='accurate'))
  EProc$sSetLocationInfo(LatDeg=lat_site, LongDeg=long_site, TimeZoneHour=tz$utc_offset_h) # why do they need the info
  ####
  ##################################################
  # USTAR filtering
  # set up seasons for some sites with fewer nighttime points
  if (name_site %in% c("CL-SDP", "US-Snd", "US-SP3")) {
    uStarTh <-
      EProc$sEstUstarThold(
        seasonFactor = usCreateSeasonFactorMonth(
          data_EddyProc$DateTime,
          month = month(data_EddyProc$DateTime),
          year = year(data_EddyProc$DateTime),
          startMonth = c(1)
        ),
        ctrlUstarSub = usControlUstarSubsetting(
          taClasses = 7,
          UstarClasses = 20,
          swThr = 10,
          minRecordsWithinTemp = 10,
          minRecordsWithinSeason = 100,
          minRecordsWithinYear = 3000,
          isUsingOneBigSeasonOnFewRecords = TRUE
        )
      )
  } else {
    uStarTh <- EProc$sEstUstarThold()
  }
  #
  # gap fill Rg, Tair, VPD, 'Ustar', TS, SWC; no need to gap fill 'PA','NETRAD', and 'H'. 
  ##################################################
    EProc$sMDSGapFill('Tair', FillAll=FALSE)
    #
    EProc$sMDSGapFill('Rg', FillAll=FALSE)
    #
    EProc$sMDSGapFill('VPD', FillAll=FALSE)
    #
    EProc$sMDSGapFill('Ustar', FillAll=FALSE)
    #
    if (col_names_used[10]) {
      EProc$sMDSGapFill('CO2', FillAll=FALSE)
    }
    #
    if (col_names_used[11]) {
      EProc$sMDSGapFill('TS', FillAll=FALSE)
    }
    #
    if (col_names_used[12]) {
      EProc$sMDSGapFill('SWC', FillAll=FALSE)
    } 

  # gap fill NEE and LE after Ustar filter; this takes minutes.
  EProc$sMDSGapFillAfterUstar('NEE')
  
  EProc$sMDSGapFillAfterUstar('LE')
  #
  # please note: I have to add Suffix.s='uStar' because we used uStar filled NEE
  if (name_site %in% c("CL-SDP", "US-xSP")) {
    error_code <- EProc$sMRFluxPartition(Suffix.s='uStar', parsE0Regression=list(TempRange = 3))  # the site has fewer data
  } else {
    error_code <- EProc$sMRFluxPartition(Suffix.s='uStar')   # night time #Reichstein 2005
  }
  if (is.null(error_code)) {
    nighttime_partition_success <- TRUE
  } else if (error_code == -111) {
    nighttime_partition_success <- FALSE
  }
  # Do daytime partition methods, but sometimes it fails. 
  daytime_partition_success <- FALSE
  tryCatch(
    {
      if (name_site %in% c("BR-Ma2", "BR-Sa1")) {
        EProc$sGLFluxPartitionUStarScens(Suffix.s='uStar', controlGLPart = partGLControl(smoothTempSensEstimateAcrossTime = FALSE))
      } else {
        EProc$sGLFluxPartition(Suffix.s='uStar') 
      }
      daytime_partition_success <- TRUE
    }, error=function(e) {
      message("Daytime partitioning does not work: ", e$message)
    }
  )
  #
  output <- EProc$sExportResults()
  #
  # deal with the abnormal nighttime partition at CA-Man: if air temperature below -5 C, GPP is set to 0.
  if (nighttime_partition_success & name_site %in% c("CA-Man", "US-Uaf")) {
    if (name_site=="CA-Man") {
      id_abnormal <- which((output$R_ref_uStar > 20  |  output$Reco_uStar > 5) & output$Tair_f < - 5)
    } else if (name_site=="US-Uaf") {
      id_abnormal <- which((output$R_ref_uStar > 35  |  output$Reco_uStar > 10) & output$Tair_f < - 5)
    }
    output$GPP_uStar_f[id_abnormal] <- 0.0
    output$Reco_uStar[id_abnormal] <- output$NEE_uStar_f[id_abnormal]
  }
  # deal with infinite respiration
  if (any(is.infinite(output$Reco_DT_uStar))) {
    id_abnormal <- which(is.infinite(output$Reco_DT_uStar))
    output$Reco_DT_uStar[id_abnormal] <- 0.0
    output$GPP_DT_uStar[id_abnormal] <- - output$NEE_uStar_f[id_abnormal]
  }
  #
  # check gap-filled data quality by looking at fingerprint of each time series.
  EProc$sPlotFingerprint("NEE_uStar_f", Format = 'pdf', Dir='plots')
  EProc$sPlotFingerprint("NEE", Format = 'pdf', Dir='plots')
  #
  EProc$sPlotFingerprint("Tair_f", Format = 'pdf', Dir='plots')
  EProc$sPlotFingerprint("Tair", Format = 'pdf', Dir='plots')  
  #
  EProc$sPlotFingerprint("VPD_f", Format = 'pdf', Dir='plots')
  EProc$sPlotFingerprint("VPD", Format = 'pdf', Dir='plots')
  #
  if (col_names_used[11]) {
    EProc$sPlotFingerprint("TS_f", Format = 'pdf', Dir='plots')
    EProc$sPlotFingerprint("TS", Format = 'pdf', Dir='plots')     
  }
  #
  if (col_names_used[12]) {
    EProc$sPlotFingerprint("SWC_f", Format = 'pdf', Dir='plots')
    EProc$sPlotFingerprint("SWC", Format = 'pdf', Dir='plots')     
  }
  if (daytime_partition_success) {
      EProc$sPlotFingerprint("GPP_DT_uStar", Format = 'pdf', Dir='plots')
      EProc$sPlotFingerprint("Reco_DT_uStar", Format = 'pdf', Dir='plots')
  }
  if (nighttime_partition_success) {
    EProc$sPlotFingerprint("GPP_uStar_f", Format = 'pdf', Dir='plots')
    EProc$sPlotFingerprint("Reco_uStar", Format = 'pdf', Dir='plots')
  }
  
  ####fill large gaps in NEE using partitioned GPP and ER
  # this is more reliable than using MDS gap fill method directly. 
  if (daytime_partition_success) {
    output$NEE_uStar_f[is.na(output$NEE_uStar_f)] <- output$Reco_DT_uStar[is.na(output$NEE_uStar_f)] - output$GPP_DT_uStar[is.na(output$NEE_uStar_f)]
  } 
  
  #####################################################################################################
  ####Output hourly, daily, and yearly gap-filled data
  a_HH <- data_BASE %>% select(c(TIMESTAMP_START, TIMESTAMP_END)) %>% filter(year(ymd_hm(TIMESTAMP_START)) >= year_first_NEE)
  # 
  # the units for hourly NEE: umolCO2 m-2 s-1
  a_HH <- a_HH %>% mutate(TA_F_MDS=output$Tair_f, TA_F_MDS_QC=output$Tair_fqc, 
                          VPD_F_MDS=output$VPD_f, VPD_F_MDS_QC=output$VPD_fqc, 
                          SW_IN_F_MDS=output$Rg_f/2.3, SW_IN_F_MDS_QC=output$Rg_fqc, # use SW_IN when putting a_HH
                          LE_F_MDS=output$LE_uStar_f, LE_F_MDS_QC=output$LE_uStar_fqc, NIGHT=output$Rg_f < 10, 
                          NEE_VUT_REF=output$NEE_uStar_f, NEE_VUT_REF_QC=output$Ustar_uStar_fqc)
  # if CO2 is in the dataset
  if (col_names_used[10]) {
    a_HH <- a_HH %>% mutate(CO2_F_MDS=output$CO2_f,	CO2_F_MDS_QC=output$CO2_fqc)
  }
  # if TS is in the dataset
  if (col_names_used[11]) {
    a_HH <- a_HH %>% mutate(TS_F_MDS_1=output$TS_f, TS_F_MDS_1_QC=output$TS_fqc)
  } 
  # if SWC is in the dataset
  if (col_names_used[12]) {
    a_HH <- a_HH %>% mutate(SWC_F_MDS_1=output$SWC_f, SWC_F_MDS_1_QC=output$SWC_fqc)
  }
  # 
  if (daytime_partition_success) {
    a_HH <- a_HH %>% mutate(GPP_DT_VUT_REF=output$GPP_DT_uStar, RECO_DT_VUT_REF=output$Reco_DT_uStar)
  }
  #
  if (nighttime_partition_success) {
    a_HH <- a_HH %>% mutate(GPP_NT_VUT_REF=output$GPP_uStar_f, RECO_NT_VUT_REF=output$Reco_uStar)
  }  
  #
  # the units for daily NEE, GPP, and RECO: gC m-2 d-1 ; TIMESTAMP=sprintf("%04d%02d%02d", )
  a_DD <- a_HH %>% mutate(YEAR=year(ymd_hm(TIMESTAMP_START)), MONTH=month(ymd_hm(TIMESTAMP_START)),     
                          DAY=day(ymd_hm(TIMESTAMP_START)), TIMESTAMP=sprintf("%04d%02d%02d", YEAR, MONTH, DAY)) %>% 
                   group_by(TIMESTAMP) %>% summarise(TA_F_MDS=mean(TA_F_MDS), VPD_F_MDS=mean(VPD_F_MDS), 
                                                     SW_IN_F_MDS=mean(SW_IN_F_MDS), LE_F_MDS=mean(LE_F_MDS), 
                                                     NEE_VUT_REF=sum(NEE_VUT_REF)*dt*0.0864*12)
  if (col_names_used[10]) {
    a_DD_CO2 <- a_HH %>% mutate(YEAR=year(ymd_hm(TIMESTAMP_START)), MONTH=month(ymd_hm(TIMESTAMP_START)),     
                               DAY=day(ymd_hm(TIMESTAMP_START)), TIMESTAMP=sprintf("%04d%02d%02d", YEAR, MONTH, DAY)) %>% 
                        group_by(TIMESTAMP) %>% summarise(CO2_F_MDS=mean(CO2_F_MDS))
    a_DD <- a_DD %>% left_join(a_DD_CO2, by="TIMESTAMP")
  }
  #
  if (col_names_used[11]) {
    a_DD_TS <- a_HH %>% mutate(YEAR=year(ymd_hm(TIMESTAMP_START)), MONTH=month(ymd_hm(TIMESTAMP_START)),     
                                DAY=day(ymd_hm(TIMESTAMP_START)), TIMESTAMP=sprintf("%04d%02d%02d", YEAR, MONTH, DAY)) %>% 
                         group_by(TIMESTAMP) %>% summarise(TS_F_MDS_1=mean(TS_F_MDS_1))
    a_DD <- a_DD %>% left_join(a_DD_TS, by="TIMESTAMP")
  }  
  #
  if (col_names_used[12]) {
    a_DD_SWC <- a_HH %>% mutate(YEAR=year(ymd_hm(TIMESTAMP_START)), MONTH=month(ymd_hm(TIMESTAMP_START)),     
                                DAY=day(ymd_hm(TIMESTAMP_START)), TIMESTAMP=sprintf("%04d%02d%02d", YEAR, MONTH, DAY)) %>% 
                         group_by(TIMESTAMP) %>% summarise(SWC_F_MDS_1=mean(SWC_F_MDS_1))
    a_DD <- a_DD %>% left_join(a_DD_SWC, by="TIMESTAMP")
  }
  #
  if (daytime_partition_success) {
    a_DD_DTP <- a_HH %>% mutate(YEAR=year(ymd_hm(TIMESTAMP_START)), MONTH=month(ymd_hm(TIMESTAMP_START)),     
                                DAY=day(ymd_hm(TIMESTAMP_START)), TIMESTAMP=sprintf("%04d%02d%02d", YEAR, MONTH, DAY)) %>% 
                         group_by(TIMESTAMP) %>% summarise(GPP_DT_VUT_REF=sum(GPP_DT_VUT_REF)*dt*0.0864*12,
                                                           RECO_DT_VUT_REF=sum(RECO_DT_VUT_REF)*dt*0.0864*12)
    a_DD <- a_DD %>% left_join(a_DD_DTP, by="TIMESTAMP")    
  }
  if (nighttime_partition_success) {
    a_DD_NTP <- a_HH %>% mutate(YEAR=year(ymd_hm(TIMESTAMP_START)), MONTH=month(ymd_hm(TIMESTAMP_START)),     
                                DAY=day(ymd_hm(TIMESTAMP_START)), TIMESTAMP=sprintf("%04d%02d%02d", YEAR, MONTH, DAY)) %>% 
                         group_by(TIMESTAMP) %>% summarise(GPP_NT_VUT_REF=sum(GPP_NT_VUT_REF)*dt*0.0864*12,
                                                           RECO_NT_VUT_REF=sum(RECO_NT_VUT_REF)*dt*0.0864*12)
    a_DD <- a_DD %>% left_join(a_DD_NTP, by="TIMESTAMP")
  }
  
  # yearly NEE, GPP, and RECO: gC m-2 year-1
  a_YY <- a_HH %>% mutate(TIMESTAMP=year(ymd_hm(TIMESTAMP_START))) %>% group_by(TIMESTAMP) %>% 
                   summarise(TA_F_MDS=mean(TA_F_MDS), VPD_F_MDS=mean(VPD_F_MDS), SW_IN_F_MDS=mean(SW_IN_F_MDS), 
                             LE_F_MDS=mean(LE_F_MDS), NEE_VUT_REF=sum(NEE_VUT_REF)*dt*0.0864*12)
  if (col_names_used[10]) {
    a_YY_CO2 <- a_HH %>% mutate(TIMESTAMP=year(ymd_hm(TIMESTAMP_START))) %>% group_by(TIMESTAMP) %>%
                        summarise(CO2_F_MDS=mean(CO2_F_MDS))
    a_YY <- a_YY %>% left_join(a_YY_CO2, by="TIMESTAMP")
  }
  if (col_names_used[11]) {
    a_YY_TS <- a_HH %>% mutate(TIMESTAMP=year(ymd_hm(TIMESTAMP_START))) %>% group_by(TIMESTAMP) %>%
                        summarise(TS_F_MDS_1=mean(TS_F_MDS_1))
    a_YY <- a_YY %>% left_join(a_YY_TS, by="TIMESTAMP")
  }
  if (col_names_used[12]) {
    a_YY_SWC <- a_HH %>% mutate(TIMESTAMP=year(ymd_hm(TIMESTAMP_START))) %>% group_by(TIMESTAMP) %>%
                        summarise(SWC_F_MDS_1=mean(SWC_F_MDS_1))
    a_YY <- a_YY %>% left_join(a_YY_SWC, by="TIMESTAMP")
  }
  if (daytime_partition_success) {
    a_YY_DTP <- a_HH %>% mutate(TIMESTAMP=year(ymd_hm(TIMESTAMP_START))) %>% group_by(TIMESTAMP) %>%
                        summarise(GPP_DT_VUT_REF=sum(GPP_DT_VUT_REF)*dt*0.0864*12, 
                                  RECO_DT_VUT_REF=sum(RECO_DT_VUT_REF)*dt*0.0864*12)
    a_YY <- a_YY %>% left_join(a_YY_DTP, by="TIMESTAMP")
  }
  if (nighttime_partition_success) {
    a_YY_NTP <- a_HH %>% mutate(TIMESTAMP=year(ymd_hm(TIMESTAMP_START))) %>% group_by(TIMESTAMP) %>%
                        summarise(GPP_NT_VUT_REF=sum(GPP_NT_VUT_REF)*dt*0.0864*12,
                                  RECO_NT_VUT_REF=sum(RECO_NT_VUT_REF)*dt*0.0864*12)
    a_YY <- a_YY %>% left_join(a_YY_NTP, by="TIMESTAMP")
  }
  #
  # Separate daytime and nighttime NEE
  NEE_DAY <- a_HH %>% filter(!NIGHT) %>% mutate(TIMESTAMP=year(ymd_hm(TIMESTAMP_START))) %>% group_by(TIMESTAMP) %>% 
                      summarise(NEE_VUT_REF_DAY=sum(NEE_VUT_REF)*dt*0.0864*12)
  NEE_NIGHT <- a_HH %>% filter(NIGHT) %>% mutate(TIMESTAMP=year(ymd_hm(TIMESTAMP_START))) %>% group_by(TIMESTAMP) %>% 
                      summarise(NEE_VUT_REF_NIGHT=sum(NEE_VUT_REF)*dt*0.0864*12)
  a_YY <- a_YY %>% left_join(NEE_DAY, by="TIMESTAMP") %>% left_join(NEE_NIGHT, by="TIMESTAMP")
  #
  #######################################################################################################
  ##adding P_ERA into hourly, daily, or yearly data sets
  if (!name_site %in% c("US-SP2")) {
    df_prec <- read.csv(files_ERA[grepl(paste0(name_site, '_tp_'), files_ERA)])
    a_HH <- a_HH %>% left_join(df_prec, by=c("TIMESTAMP_START" = "time"))
    #
    a_DD_P <- a_HH %>% mutate(YEAR=year(ymd_hm(TIMESTAMP_START)), MONTH=month(ymd_hm(TIMESTAMP_START)),     
                               DAY=day(ymd_hm(TIMESTAMP_START)), TIMESTAMP=sprintf("%04d%02d%02d", YEAR, MONTH, DAY)) %>% 
                        group_by(TIMESTAMP) %>% summarise(P_ERA=sum(P_ERA, na.rm=T))
    a_DD <- a_DD %>% left_join(a_DD_P, by="TIMESTAMP")
    #
    a_YY_P <- a_HH %>% mutate(TIMESTAMP=year(ymd_hm(TIMESTAMP_START))) %>% group_by(TIMESTAMP) %>%
                        summarise(P_ERA=sum(P_ERA, na.rm=T))
    a_YY <- a_YY %>% left_join(a_YY_P, by="TIMESTAMP")
  }
  #
# use published data to replace failed partition at CA-Cbo
if (name_site %in% c('CA-Cbo')) {
  FC_publication <- read.csv(file.path(dir_rawdata, 'publication', "publication_carbon_flux.csv"))
  FC_publication_site <- FC_publication %>% filter(site_ID == name_site) %>% select(-site_ID)
  merged_df <- left_join(a_YY, FC_publication_site, by = "TIMESTAMP", suffix = c("", ".df2"))
  a_YY <- merged_df %>%
  mutate(across(everything(), ~ coalesce(.x, merged_df[[paste0(cur_column(), ".df2")]]))) %>%
  select(names(a_YY))
}
  # 
  # write out the three files. 
  file_name <- paste0('AMF_', name_site, '_EDDYPROC_FULLSET_HH.csv')
  write.csv(a_HH, file.path(dir_rawdata, 'Ameriflux_BASE', "ReddyProcGapFill", file_name), row.names = FALSE)
  file_name <- paste0('AMF_', name_site, '_EDDYPROC_FULLSET_DD.csv')
  write.csv(a_DD, file.path(dir_rawdata, 'Ameriflux_BASE', "ReddyProcGapFill", file_name), row.names = FALSE)
  file_name <- paste0('AMF_', name_site, '_EDDYPROC_FULLSET_YY.csv')
  write.csv(a_YY, file.path(dir_rawdata, 'Ameriflux_BASE', "ReddyProcGapFill", file_name), row.names = FALSE)
  #
}
#
##################################################################################################################
# Process the site US-PFa separately, because of the tall tower and PI already gap-filled and partitioned the data. 
name_site <- "US-PFa"
data_BASE <- amf_read_base(files[grepl(name_site, files)], parse_timestamp=TRUE, unzip = T)
data_BASE[data_BASE<=-9999] <- NA
# remove data in 2024 because it is incomplete
data_BASE <- data_BASE %>% filter(YEAR < 2024)
dt <- as.numeric(difftime(data_BASE$TIMESTAMP[2], data_BASE$TIMESTAMP[1], units=c('days')))
#
# Very sparse data on TS and SWC, so I removed these variables.
# need to gap-fill VPD, LE, PPFD, and CO2
data_EddyProc <- data.frame(DateTime = ymd_hm(data_BASE$TIMESTAMP_END))
data_EddyProc <- data_EddyProc %>% mutate(NEE = data_BASE$NEE_PI_F, Tair=data_BASE$TA_PI_F_1_3_1, Ustar=data_BASE$USTAR_PI_F_1_3_1, 
                                          VPD = data_BASE$VPD_PI_F_1_3_1, LE = data_BASE$LE, Rg = data_BASE$PPFD_IN_1_1_1, 
                                          CO2 = coalesce(data_BASE$CO2_1_3_1, data_BASE$CO2_1_2_1, data_BASE$CO2_1_1_1),
                                          GPP = data_BASE$GPP_PI_F, RECO = data_BASE$RECO_PI_F)
# gap-fill solar radiation with ERA5 data
df_sw <- read.csv(files_ERA_SW[grepl(paste0(name_site, '_ssrd_'), files_ERA_SW)])
df_sw$time <- ymd_hm(df_sw$time)
df_sw <- data_EddyProc %>% select('DateTime', 'Rg') %>% left_join(df_sw, by=c("DateTime" = "time"))
df_sw <- blend_ERA5_FLUX(merged_data=df_sw, varname_FLUX='Rg', varname_ERA5='SW_ERA', blending_rule="lm_no_intercept")
data_EddyProc$Rg <- df_sw$Rg_f
#
EProc <- sEddyProc$new(name_site, data_EddyProc, c("NEE", "LE", "Rg", "Tair", "VPD", "Ustar", "CO2", "GPP", "RECO"), DTS=1/dt)
#
EProc$sMDSGapFill('VPD', FillAll=FALSE)
EProc$sMDSGapFill('LE', FillAll=FALSE)
EProc$sMDSGapFill('Rg', FillAll=FALSE)
EProc$sMDSGapFill('CO2', FillAll=FALSE)
#
output <- EProc$sExportResults()
# the PI used nighttime partition from this paper: https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2022JG007014
a_HH <- data_BASE %>% select(c(TIMESTAMP_START, TIMESTAMP_END)) %>% 
  mutate(TA_F_MDS=data_BASE$TA_PI_F_1_3_1, TA_F_MDS_QC=ifelse(is.na(data_BASE$TA_1_3_1), 1, 0), 
         VPD_F_MDS=output$VPD_f, VPD_F_MDS_QC=output$VPD_fqc, 
         SW_IN_F_MDS=output$Rg_f/2.3, SW_IN_F_MDS_QC=output$Rg_fqc, 
         LE_F_MDS=output$LE_f, LE_F_MDS_QC=output$LE_fqc, NIGHT=output$Rg_f < 10, 
         NEE_VUT_REF=data_BASE$NEE_PI_F, NEE_VUT_REF_QC=ifelse(is.na(data_BASE$NEE_PI), 1, 0), 
         CO2_F_MDS=output$CO2_f, CO2_F_MDS_QC=output$CO2_fqc, 
         GPP_NT_VUT_REF=data_BASE$GPP_PI_F, RECO_NT_VUT_REF=data_BASE$RECO_PI_F)   #
# add ERA5 precipitation data
df_prec <- read.csv(files_ERA[grepl(paste0(name_site, '_tp_'), files_ERA)])
a_HH <- a_HH %>% left_join(df_prec, by=c("TIMESTAMP_START" = "time"))

# we use NA values in our daily estimates, but not on our annual estimates
a_DD <- a_HH %>% mutate(YEAR=year(ymd_hm(TIMESTAMP_START)), MONTH=month(ymd_hm(TIMESTAMP_START)),     
                        DAY=day(ymd_hm(TIMESTAMP_START)), TIMESTAMP=sprintf("%04d%02d%02d", YEAR, MONTH, DAY)) %>% 
                 group_by(TIMESTAMP) %>% summarise(TA_F_MDS=mean(TA_F_MDS), VPD_F_MDS=mean(VPD_F_MDS), SW_IN_F_MDS=mean(SW_IN_F_MDS), 
                                                   LE_F_MDS=mean(LE_F_MDS), CO2_F_MDS=mean(CO2_F_MDS), P_ERA=sum(P_ERA, na.rm=T), 
                                                   NEE_VUT_REF=sum(NEE_VUT_REF)*dt*0.0864*12, 
                                                   GPP_NT_VUT_REF=sum(GPP_NT_VUT_REF)*dt*0.0864*12,
                                                   RECO_NT_VUT_REF=sum(RECO_NT_VUT_REF)*dt*0.0864*12)
#
a_YY <- a_HH %>% mutate(TIMESTAMP=year(ymd_hm(TIMESTAMP_START))) %>% group_by(TIMESTAMP) %>% 
                   summarise(TA_F_MDS=mean(TA_F_MDS), VPD_F_MDS=mean(VPD_F_MDS), SW_IN_F_MDS=mean(SW_IN_F_MDS), 
                             LE_F_MDS=mean(LE_F_MDS), CO2_F_MDS=mean(CO2_F_MDS), P_ERA=sum(P_ERA, na.rm=T), 
                             NEE_VUT_REF=sum(NEE_VUT_REF)*dt*0.0864*12, GPP_NT_VUT_REF=sum(GPP_NT_VUT_REF)*dt*0.0864*12,
                             RECO_NT_VUT_REF=sum(RECO_NT_VUT_REF)*dt*0.0864*12)
#
# write out the three files. 
file_name <- paste0('AMF_', name_site, '_EDDYPROC_FULLSET_HH.csv')
write.csv(a_HH, file.path(dir_rawdata, 'Ameriflux_BASE', "ReddyProcGapFill", file_name), row.names = FALSE)
file_name <- paste0('AMF_', name_site, '_EDDYPROC_FULLSET_DD.csv')
write.csv(a_DD, file.path(dir_rawdata, 'Ameriflux_BASE', "ReddyProcGapFill", file_name), row.names = FALSE)
file_name <- paste0('AMF_', name_site, '_EDDYPROC_FULLSET_YY.csv')
write.csv(a_YY, file.path(dir_rawdata, 'Ameriflux_BASE', "ReddyProcGapFill", file_name), row.names = FALSE)
#

```


```{r compare with PI partitioned data}
#
name_site <- "US-Los"    # , US-CMW, US-Los, "US-Uaf" "CA-Mer"(no pi data)
#
files <- list.files(file.path(dir_rawdata, 'Ameriflux_BASE'), pattern="zip", full.names = T)
data_BASE <- amf_read_base(files[grepl(name_site, files)], parse_timestamp=TRUE, unzip = T)
# set abnormal values (<=-9999) to NA
data_BASE[data_BASE<=-9999] <- NA
#
Ymean <- data_BASE %>% group_by(YEAR) %>% summarise(NEE_PI_F=sum(NEE_PI_F)*1/48*0.0864*12,
                                                    GPP_PI_F=sum(GPP_PI_F)*1/48*0.0864*12,
                                                    RECO_PI_F=sum(as.numeric(RECO_PI_F))*1/48*0.0864*12)
# Uaf, NEE is similar, GPP and RECO could be very different. 
# US-CMW, similar for the three variables. 
# US-Los, NEE and GPP are good, RECO by the PI may be problematic. 

```

