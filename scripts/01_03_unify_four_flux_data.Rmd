---
title: "summarize all the EC sites with long-term data"
output: pdf_document
date: "2025-02-05"
author: "Junna Wang"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r raw data location}
rm(list=ls())
# change this location where the raw data will be saved accordingly
# dir_rawdata <- '/Users/jw2946/Documents/stability_project/SiteData/'
dir_rawdata <- '/Volumes/MaloneLab/Research/Stability_Project/SiteData'
#
```


```{r all long-term sites with no less than 10-year data}
library(librarian)
shelf(tidyverse, terra)
#
data_length_thresh <- 10
#
# sites in Ameriflux FLUXNET
files_Ameri <- list.files(path =  file.path(dir_rawdata, "Ameriflux_FLUXNET"), pattern="\\.zip$")
data_length <- unlist(lapply(files_Ameri, function(x) (-as.numeric(substring(x, 28, 31))) + as.numeric(substring(x, 33, 36)) + 1))
sites_Ameri <- unlist(lapply(files_Ameri, function(x) substring(x, 5, 10)))
sites_Ameri <- sites_Ameri[data_length >= data_length_thresh]
#
# sites in FLUXNET2020 and also in ICOS FLUXNET
files_FLUXNET2020_ICOS <- list.files(path =  file.path(dir_rawdata, "ICOS_FLUXNET2025-1", "unzip_connect2020"), pattern = "_FLUXNET_HH_L2.csv$")
sites_FLUXNET2020_ICOS <- unlist(lapply(files_FLUXNET2020_ICOS, function(x) substring(x, 9, 14)))
#
# sites in FLUXNET2015 and also in ICOS FLUXNET
files_FLUXNET2015_ICOS <- list.files(path =  file.path(dir_rawdata, "ICOS_FLUXNET2025-1", "unzip_connect2015"), pattern = "_FLUXNET_HH_L2.csv$")
sites_FLUXNET2015_ICOS <- unlist(lapply(files_FLUXNET2015_ICOS, function(x) substring(x, 9, 14)))
#
# sites in FLUXNET2020 but not in ICOS FLUXNET
files_FLUXNET2020 <- list.files(path =  file.path(dir_rawdata, "FLUXNET2020"), pattern = "\\.zip$")
data_length <- unlist(lapply(files_FLUXNET2020, function(x) (-as.numeric(substring(x, 32, 35))) + as.numeric(substring(x, 37, 40)) + 1))
sites_FLUXNET2020 <- unlist(lapply(files_FLUXNET2020, function(x) substring(x, 5, 10)))
sites_FLUXNET2020 <- sites_FLUXNET2020[data_length >= data_length_thresh]
sites_FLUXNET2020 <- setdiff(sites_FLUXNET2020, sites_FLUXNET2020_ICOS)
#
# sites in FLUXNET2015 only
files_FLUXNET2015 <- list.files(path =  file.path(dir_rawdata, "FLUXNET2015_10yr_only"), pattern = "_FLUXNET2015_FULLSET_HH_")
sites_FLUXNET2015 <- unlist(lapply(files_FLUXNET2015, function(x) substring(x, 5, 10)))
#
# sites in Ameriflux BASE
files_Ameri_BASE <- list.files(path =  file.path(dir_rawdata, "Ameriflux_BASE", "ReddyProcGapFill"), pattern = "\\_HH.csv$")
sites_Ameri_BASE <- unlist(lapply(files_Ameri_BASE, function(x) substring(x, 5, 10)))
sites_Ameri_BASE <- setdiff(sites_Ameri_BASE, sites_Ameri)
#
# characteristics of these sites: climate, vegetation, MAP, MAT, Elevation
sites_long <- data.frame(site_ID = c(sites_Ameri, sites_Ameri_BASE, 
                                     sites_FLUXNET2020_ICOS, sites_FLUXNET2015_ICOS, 
                                     sites_FLUXNET2020, sites_FLUXNET2015), 
                          source = c(rep("AmeriFlux_FLUXNET", length(sites_Ameri)), 
                                     rep("AmeriFlux_BASE", length(sites_Ameri_BASE)), 
                                     rep("FLUXNET2020_ICOS", length(sites_FLUXNET2020_ICOS)), 
                                     rep("FLUXNET2015_ICOS", length(sites_FLUXNET2015_ICOS)),
                                     rep("FLUXNET2020", length(sites_FLUXNET2020)), 
                                     rep("FLUXNET2015", length(sites_FLUXNET2015)) ) )
#
# get climate and vegetation information for these sites
sites_info_Ameri <- read.csv(file.path('data', 'AmeriFlux-site-search-results-202512092232.csv'))
#
# rename some of these columns
sites_info_Ameri <- sites_info_Ameri %>% rename("Lat"="Latitude..degrees.", "Long"="Longitude..degrees.",
                                                "IGBP"="Vegetation.Abbreviation..IGBP.", 
                                                "Climate.class"="Climate.Class.Abbreviation..Koeppen.",
                                                "MAT"="Mean.Average.Temperature..degrees.C.",  
                                                "MAP"="Mean.Average.Precipitation..mm.")
#
sites_info_Europ <- read.csv(file.path("data", "European_SitesList.csv"))
#
sites_info_Europ <- sites_info_Europ %>% rename("Site.ID"="Site.Code", "Lat"="Site.Latitude",
                                                "Long"="Site.Longitude", "IGBP"="IGBP.Code", 
                                                "MAT" = "Mean.Annual.Temperature", 
                                                "MAP" = "Mean.Annual.Precpitation")
# get climate classes for these locations
climate_koppen   <- terra::rast('data/koppen_geiger_0p00833333.tif')
# plot(climate_koppen)
#
# get climate for the European coordinates
xy <- data.frame(x=sites_info_Europ$Long, y=sites_info_Europ$Lat)
#
tmp <- terra::extract(climate_koppen, xy)
#
tmp <- tmp %>% mutate(koppen = case_when(
  #
  # We do not have so many climates in the first category
  koppen_geiger_0p00833333 == 21 ~ 'Dwa', 
  koppen_geiger_0p00833333 == 22 ~ 'Dwb', 
  koppen_geiger_0p00833333 == 23 ~ 'Dwc', 
  koppen_geiger_0p00833333 == 24 ~ 'Dwd',
  #
  koppen_geiger_0p00833333 == 25 ~ 'Dfa',  
  koppen_geiger_0p00833333 == 26 ~ 'Dfb', 
  koppen_geiger_0p00833333 == 27 ~ 'Dfc', 
  koppen_geiger_0p00833333 == 28 ~ 'Dfd', 
  koppen_geiger_0p00833333 == 29 ~ 'ET',
  koppen_geiger_0p00833333 == 30 ~ 'ET',
  #
  koppen_geiger_0p00833333 == 10 ~ 'Csc',   
  koppen_geiger_0p00833333 == 9  ~ 'Csb', 
  koppen_geiger_0p00833333 == 8  ~ 'Csa',
  koppen_geiger_0p00833333 == 7  ~ 'BSk',
  koppen_geiger_0p00833333 == 6  ~ 'BSh',
  koppen_geiger_0p00833333 == 5  ~ 'BWk',  
  koppen_geiger_0p00833333 == 4  ~ 'BWh',
  #
  koppen_geiger_0p00833333 == 3  ~ 'Aw',
  koppen_geiger_0p00833333 == 2  ~ 'Am',
  koppen_geiger_0p00833333 == 1  ~ 'Af',
  #
  koppen_geiger_0p00833333 == 17  ~ 'Dsa',  
  koppen_geiger_0p00833333 == 18  ~ 'Dsb',  
  koppen_geiger_0p00833333 == 19  ~ 'Dsc',
  koppen_geiger_0p00833333 == 20  ~ 'Dsd',
  #
  koppen_geiger_0p00833333 == 14  ~ 'Cfa',
  koppen_geiger_0p00833333 == 15  ~ 'Cfb',
  koppen_geiger_0p00833333 == 16  ~ 'Cfc',  
  #
  koppen_geiger_0p00833333 == 11  ~ 'Cwa',
  koppen_geiger_0p00833333 == 12  ~ 'Cwb',
  koppen_geiger_0p00833333 == 13  ~ 'Cwc',
  #
))
#
sites_info_Europ$Climate.class <- tmp$koppen
#
common_cols <- intersect(colnames(sites_info_Europ), colnames(sites_info_Ameri))
#
sites_all <- rbind(sites_info_Ameri[common_cols], sites_info_Europ[common_cols])
#
#####correct some climate classification
sites_all$Climate.class[sites_all$Site.ID=='CA-LP1'] <- 'Dfb'
sites_all$Climate.class[sites_all$Site.ID=='US-MtB'] <- 'Csb'   # it is a bit unsure.
sites_all$Climate.class[sites_all$Site.ID=='US-MBP'] <- 'Dfb'
sites_all$Climate.class[sites_all$Site.ID=='IT-Tor'] <- 'Dfc'
#
sites_long <- sites_long %>% left_join(sites_all, by=c("site_ID" = "Site.ID")) %>% filter(IGBP!='CRO')
# 
# giving missing climate classes to US-Elm, US-Esm: Cwa
sites_long$Climate.class[sites_long$site_ID %in% c("US-Elm", "US-Esm")] <- "Cwa"
sites_long$Climate.class[sites_long$site_ID %in% c("US-Akn")] <- "Cfa"

## remove some long-term sites that are managed, disturbance affected, or data with known problems:
## CA-TP1: Ontario - Turkey Point 2002 Plantation White Pine; 
## CA-Ca2: British Columbia - Clearcut Douglas-fir stand (harvested winter 1999/2000)
## US-Rpf: Poker Flat Research Range: Succession from fire scar to deciduous forest; 
## US-Me6: this is a burned sites. 
## US-Myb: a restoration site
## CA-Ca2: clearcut around 2000
## US-KLS: actually a crop site, not a grassland site. 
## US-NC1: In 2004, 70 ha of 75 year old native hardwoods was harvested. Following the clearcut,the stand was bedded and planted with loblolly pine seedlings.
## US-NC3: The North Carolina Clearcut site #3.  
## US-Tw4: The Twitchell East End Wetland is a newly constructed restored wetland on Twitchell Island, CA. 
## US-SP2: Even aged slash pine (Pinus elliottii) plantation. Planted in Jan. 1999. data 1999-2009.
## CL-SDP: Bad quality data, especially in 2020.
## DE-Hzd: a strong storm cleared this site. 
## US-Blo: 10-year data but large gaps in some years. 
## US-NR3 and US-NR4: long time series but large gaps. Alpine tundra sites. 
## IT-Lav and IT-TrF: nighttime NEEs have problems at the two sites. 
## the US-Akn measurements actually started from 2016; removed
sites_long <- sites_long %>% filter(!site_ID %in% c('CA-TP1', 'US-Rpf', 'US-Me6', 'US-Myb', "CA-Ca2", "US-KLS", "US-NC1", "US-NC3", "US-Tw4", "US-SP2", "CL-SDP", "DE-Hzd", "US-Blo", "US-NR3", "US-NR4", "IT-Lav", "IT-TrF", "US-Akn"))
##
```


```{r remove some years}
##this chunk generates the years we should remove; 
# it takes about 1 hour.
data_length_thresh <- 10
#
files.unzip.Ameri <- list.files(file.path(dir_rawdata, "Ameriflux_FLUXNET", "unzip"), pattern = '', full.names = T)
files.unzip.FLUXNET2020 <- list.files(file.path(dir_rawdata, "FLUXNET2020", "unzip"), full.names = T)
files.unzip.FLUXNET2015 <- list.files(file.path(dir_rawdata, "FLUXNET2015_10yr_only"), full.names = T)
files.unzip.FLUXNET2020_ICOS <- list.files(file.path(dir_rawdata, "ICOS_FLUXNET2025-1", "unzip_connect2020"), full.names = T)
files.unzip.FLUXNET2015_ICOS <- list.files(file.path(dir_rawdata, "ICOS_FLUXNET2025-1", "unzip_connect2015"), full.names = T)
files.Ameri.ReddyProcGapFill <- list.files(file.path(dir_rawdata, "Ameriflux_BASE", "ReddyProcGapFill"), full.names=T)
#
df_file_structure <- data.frame(sources = c("AmeriFlux_FLUXNET", "AmeriFlux_BASE", 
                                           "FLUXNET2020_ICOS", "FLUXNET2015_ICOS", "FLUXNET2020", "FLUXNET2015"), 
                               folders = I(list(list(files.unzip.Ameri), list(files.Ameri.ReddyProcGapFill), 
                                           list(files.unzip.FLUXNET2020_ICOS), list(files.unzip.FLUXNET2015_ICOS),
                                           list(files.unzip.FLUXNET2020), list(files.unzip.FLUXNET2015))), 
                               HHfiles = c("_FLUXNET_FULLSET_(HR|HH)", "_EDDYPROC_FULLSET_HH",
                                           "_FLUXNET_HH", "_FLUXNET_HH", 
                                           "_FLUXNET2015_FULLSET_(HR|HH)", "_FLUXNET2015_FULLSET_(HR|HH)"), 
                               DDfiles = c("_FLUXNET_FULLSET_DD", "_EDDYPROC_FULLSET_DD",
                                           "_FLUXNET_DD", "_FLUXNET_DD", 
                                           "_FLUXNET2015_FULLSET_YY", "_FLUXNET2015_FULLSET_YY"),
                               YYfiles = c("_FLUXNET_FULLSET_YY", "_EDDYPROC_FULLSET_YY", 
                                           "_FLUXNET_YY", "_FLUXNET_YY", 
                                           "_FLUXNET2015_FULLSET_YY", "_FLUXNET2015_FULLSET_YY"))
#
rm_years <- data.frame(site_ID=character(), years=list())
rm_sites <- c()
#
for (i in 1:nrow(sites_long)) {
  # i = 41
  print(i)
  site_ID <- sites_long$site_ID[i]
  print(site_ID)
  folder <- unlist(df_file_structure$folders[df_file_structure$sources == sites_long$source[i]])
  HHfile <- df_file_structure$HHfiles[df_file_structure$sources == sites_long$source[i]]
  YYfile <- df_file_structure$YYfiles[df_file_structure$sources == sites_long$source[i]]
  a_HH <- read.csv(folder[grepl(pattern=paste0(site_ID, HHfile), folder)])
  a_YY <- read.csv(folder[grepl(pattern=paste0(site_ID, YYfile), folder)])
  #
  a_HH[a_HH<=-9999] <- NA
  a_YY[a_YY<=-9999] <- NA
  #
  #### The first rule to remove years: the percentage of measured data < 0.2 of the average percentage of measured data; this is a very loose rule. 
  ## for each year, get the percentage of time when data is interpolated. 
  a_HH$TIMESTAMP <- ymd_hm(a_HH$TIMESTAMP_START) + (ymd_hm(a_HH$TIMESTAMP_END) - ymd_hm(a_HH$TIMESTAMP_START)) / 2.0
  f.data.measured <- a_HH %>% mutate(YEAR=year(TIMESTAMP)) %>% group_by(YEAR) %>% summarise(f=sum(NEE_VUT_REF_QC==0) / n())
  # print(f.data.measured)
  #
  tmp <- f.data.measured %>% filter(f > 0.1 & !is.na(f))
  meanf <- mean(tmp$f)
  #
  tmp <- f.data.measured$YEAR[is.na(f.data.measured$f) | f.data.measured$f < min(0.1, meanf/5)]
  # print(tmp)
  #
  #### The second rule to remove years: there are NA values in yearly data. 
  tmp <- c(tmp, a_YY$TIMESTAMP[is.na(a_YY$NEE_VUT_REF)])
  #
  #### The third rule to remove years with extreme NEE outliers: a year's NEE outside of 3 * sd, and data is less than the average. 
  # detect outliers: Using the Interquartile Range (IQR) Method, NOT the sd method
  Q1 <- quantile(a_YY$NEE_VUT_REF, 0.25, na.rm=T)
  Q3 <- quantile(a_YY$NEE_VUT_REF, 0.75, na.rm=T)
  IQR <- max(Q3 - Q1, 100)
  outlier <- !between(a_YY$NEE_VUT_REF, Q1 - 1.5 * IQR, Q3 + 1.5 * IQR)
  tmp <- c(tmp, a_YY$TIMESTAMP[which(outlier & f.data.measured$f < meanf * 0.5)])
  #
  outlier_extreme <- !between(a_YY$NEE_VUT_REF, Q1 - 3 * IQR, Q3 + 3 * IQR)
  tmp <- c(tmp, a_YY$TIMESTAMP[which(outlier_extreme & f.data.measured$f < meanf * 0.75)])   # have to have enough observations.
  #
  #### the fourth rule: if the outlier is in the first year, discard it; the first year measurement has lots of uncertainties.
  if (!is.na(outlier[1]) & outlier[1] & f.data.measured$f[1] < 0.5) {
    tmp <- c(tmp, a_YY$TIMESTAMP[1])
  }
  #
  #### the fifth rule: the first and the last year must have at least 50% of average data
  # year1 <- min((setdiff(a_YY$TIMESTAMP, unique(tmp))))
  # if (f.data.measured$f[a_YY$TIMESTAMP==year1] < meanf * 0.5) {
  #   tmp <- c(tmp, year1)
  # }
  # year1 <- max((setdiff(a_YY$TIMESTAMP, unique(tmp))))
  # if (f.data.measured$f[a_YY$TIMESTAMP==year1] < meanf * 0.5) {
  #   tmp <- c(tmp, year1)
  # }
  #
  #### the six rule: there should be enough monthly variations
  #### this does not apply to tropical sites. 
  if (!site_ID %in% c("BR-Sa1", "BR-Ma2", "GF-Guy")) {
    monthly_NEE_day <- a_HH %>% mutate(YEAR=year(TIMESTAMP), MONTH=month(TIMESTAMP)) %>%
      group_by(YEAR, MONTH) %>% summarise(NEE=mean(NEE_VUT_REF, na.rm=T))
    # remove incomplete years
    monthly_NEE_day <- monthly_NEE_day %>% group_by(YEAR) %>% summarise(n=n()) %>% merge(monthly_NEE_day, by="YEAR") %>% filter(n==12) %>% select(-n)
    #
    monthly_NEE_day_pattern <- monthly_NEE_day %>% filter(!YEAR %in% unique(tmp)) %>% group_by(MONTH) %>%
      summarise(Q2=quantile(NEE, 0.5, na.rm=T), Q1=quantile(NEE, 0.25, na.rm=T), Q3=quantile(NEE, 0.75, na.rm=T)) %>%
      mutate(lowlimit=Q1 - 2.0 * (Q3 - Q1), uplimit=Q3 + 2.0 * (Q3 - Q1))
    #
    monthly_NEE_var <- monthly_NEE_day %>% group_by(YEAR) %>% summarise(sd=sd(NEE, na.rm=T)) %>% 
      filter(sd < (sd(monthly_NEE_day_pattern$Q1)+sd(monthly_NEE_day_pattern$Q3))/10)
    if (nrow(monthly_NEE_var) > 0) {
      tmp <- c(tmp, monthly_NEE_var$YEAR)
      print(paste0('remove no monthly variation: ', site_ID, monthly_NEE_var$YEAR))
    }
  }
  #
  #### the seventh rule: monthly average NEE should be within a bound
  monthly_NEE_range <- monthly_NEE_day %>% left_join(monthly_NEE_day_pattern[,c(1,5:6)], by="MONTH") %>% filter(NEE > uplimit | NEE < lowlimit) %>% group_by(YEAR) %>% summarise(n=n()) %>% filter(n>=3)
  #### a year has >= 1 month outliers and an outlier year
  year1 <- intersect(monthly_NEE_range$YEAR, a_YY$TIMESTAMP[which(outlier)])
  if (length(year1) > 0) {
    tmp <- c(tmp, year1)
    print(paste0('remove monthly outliters: ', site_ID, year1))
  }
  #
  #### the eighth rule: monthly average NEE should be similar to average seasonal patterns ####
  #### only applies sites with seasonal pattern, not in tropical and subtropical
  if (!site_ID %in% c("BR-Sa1", "BR-Ma2", "US-LL3", "US-LL1", "US-LL2", "GF-Guy", "CL-SDF", "US-SP1", "US-SP3")) {
    monthly_NEE_var <- monthly_NEE_day %>% group_by(YEAR) %>% summarise(cor=cor(NEE, monthly_NEE_day_pattern$Q2)) 
    Q1 <- as.numeric(quantile(monthly_NEE_var$cor, 0.25, na.rm=T))
    Q3 <- as.numeric(quantile(monthly_NEE_var$cor, 0.75, na.rm=T))
    monthly_NEE_var_extreme <- monthly_NEE_var %>% filter(!between(cor, Q1 - 3*(Q3-Q1), Q3 + 3*(Q3-Q1)) & cor < 0.4 | is.na(cor))
    if (nrow(monthly_NEE_var_extreme) > 0) {
      tmp <- c(tmp, monthly_NEE_var_extreme$YEAR)
      print(paste0('remove monthly extreme low correlation: ', site_ID, monthly_NEE_var_extreme$YEAR))
    }
    #
    monthly_NEE_var_extreme <- monthly_NEE_var %>% left_join(f.data.measured, by='YEAR') %>% 
      filter(!between(cor, Q1 - 1.5*(Q3-Q1), Q3 + 1.5*(Q3-Q1)) & f < 0.75 * meanf & cor < 0.4)
    if (nrow(monthly_NEE_var_extreme) > 0) {
      tmp <- c(tmp, monthly_NEE_var_extreme$YEAR)
      print(paste0('remove monthly low correlation and less data: ', site_ID, monthly_NEE_var_extreme$YEAR))
    }
  }
  ## data in 2009 at US-KL2, US-KL3, US-KM2, US-KM3, US-KM4 should not be used. 
  ## due to land use change from soybean to natural land cover in 2009.
  if (site_ID %in% c("US-KL2", "US-KL3", "US-KM2", "US-KM3", "US-KM4")) {
    tmp <- c(tmp, 2009)
  }
  if (site_ID == "US-Rws") {
    # use the first year data at this site, because that year's data is consistent with overall trend, and we need sites in dry climate. 
    tmp <- c()
  }
  #
  #########process the removed years, and put them into dataframe
  tmp <- unique(tmp)
  print(tmp)
  #
  if (length(tmp) > 0) {
    rm_years <- rbind(rm_years, data.frame(site_ID=site_ID, years=tmp))
  }
  a_YY_good <- a_YY %>% filter(!TIMESTAMP %in% tmp)
  plot(a_YY_good$TIMESTAMP, a_YY_good$NEE_VUT_REF, main=site_ID)
  #
  # also check partitioned GPP data
  if (!site_ID %in% c("BR-Sa1", "US-NC2", "US-SP3", "US-WCr", "US-Ho1", "US-Ho2", "US-Kon", "US-MOz", "CH-Lae", "US-NC4", "CA-Man", "US-LL1", "US-LL2", "US-PFa", "US-Uaf")) {
    plot(a_YY_good$TIMESTAMP, a_YY_good$GPP_DT_VUT_REF, main=site_ID)
  } else {
    plot(a_YY_good$TIMESTAMP, a_YY_good$GPP_NT_VUT_REF, main=site_ID)
  }
  # remove the sites without enough data
  if (a_YY_good$TIMESTAMP[nrow(a_YY_good)] - a_YY_good$TIMESTAMP[1] < data_length_thresh) {
    rm_sites <- c(rm_sites, site_ID)
  }
}
##
sites_long <- sites_long %>% filter(!site_ID %in% rm_sites)
write.csv(rm_years, file = file.path('data', 'years_to_remove.csv'), row.names = F)
##

```


```{r calculate ET and PET}
# it takes about 1 hour.
####
#
# the threshold of P/ETP: > 0.65 wet regions, 0.5-0.65 mediate regions, and < 0.5 dry regions.
# aridity index calculation using PenmanMonteith
library(librarian)
shelf(amerifluxr, Evapotranspiration)
data(constants)
#
# sites_long <- read.csv('data/long_term_ec_sites.csv')
#
# solar radiation should have the unit mega-joule / m2
# 1 W = 1 J/s; daily data is the average of subhourly data
# I need to get Tmin, Tmax, RHmin, RHmax, uz (ERA, the height wind speed is measured at 10m), u2, n or Cd, solar radiation;
#
files.unzip.Ameri <- list.files(file.path(dir_rawdata, "Ameriflux_FLUXNET", "unzip"), full.names = T)
files.unzip.Europ2020 <- list.files(file.path(dir_rawdata, "FLUXNET2020", "unzip"), full.names = T)
files.unzip.Europ2023 <- list.files(file.path(dir_rawdata, "ICOS_FLUXNET2025-1", "unzip_connect2020"), full.names = T)
files.Ameri.ReddyProcGapFill <- list.files(file.path(dir_rawdata, "Ameriflux_BASE", "ReddyProcGapFill"), full.names=T)
files.Ameri <- list.files(file.path(dir_rawdata, "Ameriflux_BASE"), pattern=".zip$", full.names=T)
#
# water characteristics
water_flux <- data.frame(site_ID=character(), PET=double(), AET=double(), AI=double())
#1:nrow(sites_long)
for (i in 1:nrow(sites_long)) {
#  i = 9
#  if (i %in% c(15, 16, 35)) {next}  # 16, 35, 
  print(i)
  site_ID <- sites_long$site_ID[i]
  water_flux[i,1] <- site_ID
  # AmeriFlux and European flux; Do I need to differentiate them? Hopefully they use the same notation
  if (sites_long$source[i] == "AmeriFlux_FLUXNET") {
    if (site_ID %in% c("US-Ha1", "US-MMS", "US-Ne1", "US-Cop")) {
      a_HH <- read.csv(files.unzip.Ameri[grepl(pattern=paste0(site_ID, "_FLUXNET_FULLSET_HR"), files.unzip.Ameri)])
    } else {
      a_HH <- read.csv(files.unzip.Ameri[grepl(pattern=paste0(site_ID, "_FLUXNET_FULLSET_HH"), files.unzip.Ameri)])
    }
    a_YY <- read.csv(files.unzip.Ameri[grepl(pattern=paste0(site_ID, "_FLUXNET_FULLSET_YY"), files.unzip.Ameri)])
  } else if (sites_long$source[i] == "EuropFlux_FLUXNET2020") {
    a_HH <- read.csv(files.unzip.Europ2020[grepl(pattern=paste0(site_ID, "_FLUXNET2015_FULLSET_HH"), files.unzip.Europ2020)])
    a_YY <- read.csv(files.unzip.Europ2020[grepl(pattern=paste0(site_ID, "_FLUXNET2015_FULLSET_YY"), files.unzip.Europ2020)])
  } else if (sites_long$source[i] == "EuropFlux_FLUXNET2023") {
    a_HH <- read.csv(files.unzip.Europ2023[grepl(pattern=paste0(site_ID, "_FLUXNET_HH"), files.unzip.Europ2023)])
    a_YY <- read.csv(files.unzip.Europ2023[grepl(pattern=paste0(site_ID, "_FLUXNET_YY"), files.unzip.Europ2023)])
  } else if (sites_long$source[i] == "AmeriFlux_BASE") {
    a_HH <- read.csv(files.Ameri.ReddyProcGapFill[grepl(pattern=paste0(site_ID, "_EDDYPROC_FULLSET_HH"), files.Ameri.ReddyProcGapFill)])
    a_YY <- read.csv(files.Ameri.ReddyProcGapFill[grepl(pattern=paste0(site_ID, "_EDDYPROC_FULLSET_YY"), files.Ameri.ReddyProcGapFill)])
    a_BASE <- amf_read_base(files.Ameri[grepl(site_ID, files.Ameri)], parse_timestamp=TRUE, unzip = T)
    a_BASE[a_BASE<=-9999] <- NA
    if (!site_ID %in% c("US-Los")) {
      RHname <- sort(names(a_BASE)[grepl("RH", names(a_BASE))])[1]
      WSname <- sort(names(a_BASE)[grepl("WS", names(a_BASE))])[1]
      print(WSname)
      a_HH <- a_HH %>% merge(a_BASE[, c("TIMESTAMP_START", RHname, WSname)], by="TIMESTAMP_START", all.x=T) %>% 
        rename("RH" = RHname, "WS_F" = WSname)
    } else {
      # calculate RH from VPD
      a_HH$RH <- pmax(100 - a_HH$VPD_F_MDS / 10 / (0.6108 * exp(17.27 * a_HH$TA_F_MDS / (a_HH$TA_F_MDS + 237.3))) * 100, 10.0)
      a_HH <- a_HH %>% merge(a_BASE[, c("TIMESTAMP_START", "WS_1_1_1")], by="TIMESTAMP_START", all.x=T) %>% 
        rename("WS_F" = "WS_1_1_1")
    }
    # in this data set SW_IN represent PPFD_IN, so convert PPFD_IN to SW_IN to be consistent with FLUXNET
    a_HH$SW_IN_F_MDS <- a_HH$SW_IN_F_MDS / 2.3 
  }
  a_HH[a_HH==-9999] <- NA
  a_YY[a_YY==-9999] <- NA
  #
  a_HH$TIMESTAMP <- ymd_hm(a_HH$TIMESTAMP_START) + (ymd_hm(a_HH$TIMESTAMP_END) - ymd_hm(a_HH$TIMESTAMP_START)) / 2.0
  # aggregate these data into average hourly within a pattern year
  climate_hourly <-
    a_HH %>% mutate(
      Month = month(a_HH$TIMESTAMP),
      Day = day(a_HH$TIMESTAMP),
      Hour = hour(a_HH$TIMESTAMP),
      Rs = a_HH$SW_IN_F_MDS * 0.0864,      # convert SW_IN to MJ m-2 day-1
      RH = a_HH$RH,
      uz = a_HH$WS_F
    ) %>%
    group_by(Month, Day, Hour) %>%
    summarise(
      Temp = mean(TA_F_MDS, na.rm = T),
      Rs = mean(Rs, na.rm = T),
      RH = mean(RH, na.rm = T),
      uz = mean(WS_F, na.rm = T)
    ) %>% mutate(Year=2000)

  ####
  # climate_hourly <- data.frame(Year=year(a_HH$TIMESTAMP), Month=month(a_HH$TIMESTAMP), Day=day(a_HH$TIMESTAMP), Hour=hour(a_HH$TIMESTAMP), Minute=minute(a_HH$TIMESTAMP)/60, Temp=a_HH$TA_F_MDS, Rs = a_HH$SW_IN_F_MDS*0.0864, RH=a_HH$RH, uz=a_HH$WS_F)
  # climate_hourly <- climate_hourly %>% group_by(Year, Month, Day, Hour) %>% summarise(Temp=mean(Temp,na.rm=T), Rs=mean(Rs,na.rm=T), RH=mean(RH,na.rm=T), uz=mean(uz,na.rm=T))
#  climate_hourly$Tdew <- climate_hourly$Temp - (100 - climate_hourly$RH) / 5.0
#  
  #### Rs - incoming solar radiation in Megajoules per square metres per day: "Tdew";        
  mydata_PET <- ReadInputs(varnames=c("Temp", "RH", "Rs", "uz"), climate_hourly, constants, stopmissing=c(50,50,50), timestep = "subdaily",
      interp_missing_days = TRUE,
      interp_missing_entries = TRUE,
      interp_abnormal = TRUE,
      missing_method = 'DoY average',
      abnormal_method = 'DoY average',
      message = "yes")
  ####Penman-Monteith FAO56 Reference Crop ET
  if (sites_long$IGBP[i] %in% c("WET", "GRA", "OSH")) {
    try(results_crop <- ET.PenmanMonteith(mydata_PET, constants, ts='daily', solar='data', crop = "short", save.csv="no"))  ###reference crop ET; short=grass, tall=standard crop; 
  } else {
    if (sites_long$MAT[i] > 5) {
      #
      try(results_crop <- ET.PenmanMonteith(mydata_PET, constants, ts='daily', solar='data', crop = "tall", save.csv="no"))  ###reference crop ET; short=grass, tall=standard crop;
    } else {
      #  
      try(results_crop <- ET.PriestleyTaylor(mydata_PET, constants, ts='daily', solar='data', alpha=0.23,
message="yes", AdditionalStats="yes", save.csv="no"))  ###reference crop ET; short=grass, tall=standard crop; 
    }
  }
  # result_PT <- ET.PriestleyTaylor(mydata_PET, constants, ts="daily",
  # solar="data", alpha=0.23, message="yes", AdditionalStats="yes", save.csv="no") 
#### get the actual ET values:
  # conversion of LE = λ ET, λ=(2.501−0.00237∗Tair)^106; LE: W/m2; ET: kg m-2 s-1
  # lambda latent heat of evaporisationin = 2.45 MJ.kg^-1 at 20 degree Celcius,
  lambda <- 2.501 - 0.00237 * mean(a_YY$TA_F_MDS, na.rm=T)
  water_flux$AET[i] <- mean(a_YY$LE_F_MDS / lambda * 0.0864 * 365.25, na.rm=T)
  water_flux$PET[i] <- mean(results_crop$ET.Annual,  na.rm=T)
}
water_flux$AI <- water_flux$AET / water_flux$PET
#####what is the best method for PET calculation in the field? 
write.csv(water_flux, file.path('data', 'water_flux_AI.csv'), row.names = F)
#
```


```{r dry, wet, and cold sites classification}
#
water_flux <- read.csv(file.path('data', 'water_flux_AI.csv'))
#
if (!"AI" %in% names(sites_long)) {
  sites_long <- sites_long %>% left_join(water_flux, by='site_ID')
}
#
sites_long <- sites_long %>% 
  mutate(category=case_when(Climate.class %in% c("Csa", "Csb", "Bsh", "Bsk", "BWh", "Bwk") & AI < 0.5 ~ 'dry',
                            Climate.class %in% c("Csa", "Csb", "Bsh", "Bsk", "BWh", "Bwk") & AI >= 0.5 ~ 'wet', 
                            Climate.class %in% c("Am", "Cwa", "Af") ~ "wet", 
                            Climate.class %in% c("Dfa", "Dfb", "Dfc", "Dfd", "ET", "Dwc") ~ "cold",
                            Climate.class %in% c("Cfa") ~ "wet",
                            Climate.class %in% c("Cfb") ~ "cold"         # those sites has winters and in high latitudes
                            ))
#
# sites_long <- sites_long %>% mutate(category=case_when(Climate.class %in% c("Dfa", "Dfb", "Dfc", "Dfd", "ET", "Dwc")  ~ 'cold',
#                              !Climate.class %in% c("Dfa", "Dfb", "Dfc", "Dfd", "ET", "Dwc") & AI < 0.4 ~ 'dry',
#                              !Climate.class %in% c("Dfa", "Dfb", "Dfc", "Dfd", "ET", "Dwc") & AI >= 0.4 ~ 'wet', 
#                               Climate.class %in% c("Am", "Cwa") ~ "wet", 
#                               Climate.class %in% c("Bsh") ~ "dry"))
# sites_long$category[is.na(sites_long$category) & sites_long$Climate.class %in% c('Cfa', 'Cfb')] <- 'wet'
# sites_long$category[is.na(sites_long$category) & sites_long$Climate.class %in% c('Csa')] <- 'dry'
#

sites_long$category[sites_long$site_ID %in% c("US-Rls", "US-Rms", "US-Rwf", "US-Rws")] = 'dry'

sum(sites_long$category=='dry', na.rm = T)
# we have 20 dry sites.
sum(sites_long$category=='wet', na.rm = T)
# we have 24 wet sites.
sum(sites_long$category=='cold', na.rm = T)
# we have 77 cold sites. 
write.csv(sites_long, file = 'data/long_term_ec_sites.csv', row.names = F)

```
