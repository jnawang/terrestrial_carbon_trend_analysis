---
title: "Convert downloaded ERA5 to csv files"
output: pdf_document
date: "2025-03-06"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r raw data location}
rm(list=ls())
# change this location where the raw data will be saved accordingly
# dir_rawdata <- '/Users/jw2946/Documents/stability_project/SiteData/'
dir_rawdata <- '/Volumes/MaloneLab/Research/Stability_Project/SiteData'
#
library(librarian)
shelf(tidyverse, ncdf4, lubridate)
```


```{r a function to get utc offset}

utc_offset <- function(lat, lon) {
  # Get timezone for the given coordinates
  timezone <- tz_lookup_coords(lat = lat, lon = lon, method = "accurate")
  #
  if (is.na(timezone)) {
    stop("Timezone not found for given coordinates.")
  }
  
  # Get the current time in UTC
  if (lat >= 0 ) {
    time_utc <- "2025-01-01 00:00:00 UTC"
  } else {
    time_utc <- "2025-07-01 00:00:00 UTC"
  }
#  print(time_utc)
  # Convert UTC time to local time with proper timezone
  local_time_result <- with_tz(time_utc, tzone = timezone)  # Keeps time zone
# print(local_time_result)
  #
  conversion <- force_tz(local_time_result, "UTC")
#  print(conversion)
  # Calculate the difference in hours, respecting time zones
  offset_hours <- as.numeric(difftime(conversion, time_utc, units = "hours"))
  #
#  print(offset_hours)
  return(offset_hours)
}
#
utc_offset(40, -100)

```

```{r read era5 data, convert it to csv files}
#### read nc files
## read each file, get long, lat, var, and years
nc_files <- list.files(file.path(dir_rawdata, 'ERA5_Tair_Prec'), pattern='.nc$')
##
sites_Ameri <- read.csv('../data/AmeriFlux-site-search-results-202501071742.csv')
sites_Ameri <- sites_Ameri[, c("Site.ID", "Longitude..degrees.", "Latitude..degrees.")]
names(sites_Ameri) <- c("site_ID", "Long", "Lat")
##
df <- data.frame(Long=double(), Lat=double(), Var=character(), site_ID=character(), file_ID=integer())
##
for (i in 1:length(nc_files)) {
#  i = 35
  nc_file <- nc_open(file.path(dir_rawdata, 'ERA5_Tair_Prec', nc_files[i]))
  df[i,3] <- names(nc_file$var)[3]
  df$Long[i] <- ncvar_get(nc_file, "longitude")
  df$Lat[i]  <- ncvar_get(nc_file, "latitude")
  df$file_ID[i] <- i
  # 
  id <- sites_Ameri$site_ID[abs(df$Long[i] - sites_Ameri$Long) < 0.02 & abs(df$Lat[i] - sites_Ameri$Lat) < 0.02]
  if (length(id) == 1) {
    df$site_ID[i] <- id
    print(paste0('find', id[1]))
  } else if (length(id) > 1 & id[1]=="US-LL2") {
    df$site_ID[i] <- id[1]
    print(id)    
  } else if (length(id) > 1 & id[1]=="CL-SDF") {
    df$site_ID[i] <- id[2]
    print(id)    
  }
  #
}
#######
df <- df %>% na.omit %>% arrange(site_ID)
sites_BASE <- unique(df$site_ID)
#
for (site in sites_BASE) {
  df_prec <- data.frame()
  df_sub <- subset(df, site_ID == site)
  hour_offset <- utc_offset(df_sub$Lat[1], df_sub$Long[1])
  #
  for (i in 1:nrow(df_sub)) {
    nc_file <- nc_open(file.path(dir_rawdata, 'ERA5_Tair_Prec', nc_files[df_sub$file_ID[i]]))
    time <- ncvar_get(nc_file, "valid_time")
    prec <- ncvar_get(nc_file, "tp")  # precipitation unit m
    #
    # convert to UTC time
    time_origin <- ncatt_get(nc_file, "valid_time", "units")$value
    #
    time_converted <- as.POSIXct(time, origin = gsub("seconds since ", "", time_origin), tz = "UTC")
    #
    # convert to local time
    df_prec <- rbind(df_prec, data.frame(time=time_converted+hour_offset*3600, prec=prec*1000))
    #
  }
  print(paste0("Check duplicates for ", site, ": ", sum(duplicated(df_prec$time))))
  df_prec <- df_prec %>% arrange(time)
  #
  # start from the first time of a year and the end time of a year
  df_prec_annual <- df_prec %>% mutate(year=year(time)) %>% group_by(year) %>% summarise(n=n()) %>% filter(n>100)
  year_start <- df_prec_annual$year[1]
  year_end   <- df_prec_annual$year[nrow(df_prec_annual)]
  #
  df_prec <- df_prec %>% filter(between(year(time), year_start, year_end))
  # 
  # convert this to sub-hourly data
  df_output <- data.frame(time=seq(df_prec$time[1], df_prec$time[nrow(df_prec)] + 30*60, by = "30 min"))
  # 
  # divided by 2?
  df_output$P_ERA <- rep(df_prec$prec/2, each=2)
  #
  # years I need to add missing data: check the first year
  time_start <- make_datetime(year = year_start, month = 1, day = 1, hour = 0, min = 0, sec = 0)
  if (df_prec$time[1] > time_start) {
    df_missed <- data.frame(time=seq(time_start, df_output$time[1] - 30*60, by = "30 min"), P_ERA=0)
    df_output <- rbind(df_missed, df_output)
  }
  #
  # years I need to add missing data: check the last year
  end_time <- make_datetime(year = year_end, month = 12, day = 31, hour = 23, min = 30, sec = 0)
  if (df_output$time[nrow(df_output)] < end_time) {
    df_missed <- data.frame(time=seq(df_output$time[nrow(df_output)] + 30*60, end_time, by = "30 min"), P_ERA=0)
    df_output <- rbind(df_output, df_missed)
  }
  # convert date time to numbers
  df_output$time <- format(df_output$time, "%Y%m%d%H%M")
  
  # save to csv files: site_ID, var, year_start, year_end
  write.csv(df_output, file=file.path(dir_rawdata, 'ERA5_Tair_Prec', paste0(site, "_", df_sub$Var[1], "_", year_start, "_", year_end, ".csv")), row.names = F)
}

```

